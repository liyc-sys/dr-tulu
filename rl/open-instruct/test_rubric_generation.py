#!/usr/bin/env python3
"""
æµ‹è¯•adaptive rubricç”ŸæˆåŠŸèƒ½
è¿™ä¸ªè„šæœ¬æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„rubricç”Ÿæˆï¼Œä½†ä¸éœ€è¦è¿è¡Œå®Œæ•´çš„è®­ç»ƒ
"""
import os
import sys
import asyncio

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä»train_dr_tulu.shå¤åˆ¶ï¼‰
os.environ["http_proxy"] = "http://httpproxy.glm.ai:8888"
os.environ["https_proxy"] = "http://httpproxy.glm.ai:8888"
os.environ["no_proxy"] = "127.0.0.1,localhost,platform.glm.ai,::1"
os.environ["OPENAI_API_KEY"] = "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"
os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
os.environ["RUBRIC_JUDGE_MODEL"] = "gpt-4.1-mini"
os.environ["RUBRIC_GENERATION_MODEL"] = "gpt-4.1-mini"

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from open_instruct.search_rewards.utils.rubric_utils import generate_instance_wise_adaptive_rubrics


async def test_rubric_generation_simple():
    """æµ‹è¯•1: ç®€å•çš„rubricç”Ÿæˆ"""
    print("=" * 60)
    print("æµ‹è¯•1: ç®€å•é—®é¢˜çš„Rubricç”Ÿæˆ")
    print("=" * 60)
    
    question = "What is the capital of France?"
    responses = [
        "The capital of France is Paris.",
        "Paris is the capital city of France, known for the Eiffel Tower.",
        "France's capital is Paris, a major European city.",
    ]
    
    print(f"\né—®é¢˜: {question}")
    print(f"å“åº”æ•°é‡: {len(responses)}")
    
    try:
        result = await generate_instance_wise_adaptive_rubrics(
            question=question,
            response_list=responses,
            existing_rubrics=None,
            model_name=os.environ.get("RUBRIC_GENERATION_MODEL", "gpt-4.1-mini")
        )
        
        if result is None:
            print("âŒ Rubricç”Ÿæˆè¿”å›None")
            return False
        else:
            print("âœ… Rubricç”ŸæˆæˆåŠŸ!")
            print(f"ç»“æœ: {result}")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_rubric_generation_with_existing():
    """æµ‹è¯•2: å¸¦æœ‰existing rubricsçš„ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: å¸¦æœ‰Existing Rubricsçš„ç”Ÿæˆ")
    print("=" * 60)
    
    question = "Explain quantum entanglement."
    responses = [
        "Quantum entanglement is when particles are connected.",
        "It's a quantum phenomenon where particles remain connected regardless of distance.",
    ]
    
    existing_rubrics = """
    - Accuracy: Response must be scientifically accurate
    - Clarity: Explanation should be clear and understandable
    """
    
    print(f"\né—®é¢˜: {question}")
    print(f"Existing rubrics: {existing_rubrics}")
    
    try:
        result = await generate_instance_wise_adaptive_rubrics(
            question=question,
            response_list=responses,
            existing_rubrics=existing_rubrics,
            model_name=os.environ.get("RUBRIC_GENERATION_MODEL", "gpt-4.1-mini")
        )
        
        if result is None:
            print("âŒ Rubricç”Ÿæˆè¿”å›None")
            return False
        else:
            print("âœ… Rubricç”ŸæˆæˆåŠŸ!")
            print(f"ç»“æœ: {result}")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_litellm_direct():
    """æµ‹è¯•3: ç›´æ¥æµ‹è¯•litellmè¿æ¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: ç›´æ¥æµ‹è¯•LiteLLMè¿æ¥")
    print("=" * 60)
    
    from open_instruct.search_rewards.utils.run_utils import run_litellm_async
    
    try:
        response = await run_litellm_async(
            model_name="gpt-4.1-mini",
            user_prompt="Say 'Hello' and nothing else.",
            max_tokens=10,
            timeout=30
        )
        
        if response == "":
            print("âŒ LiteLLMè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆè¿æ¥å¤±è´¥ï¼‰")
            return False
        else:
            print(f"âœ… LiteLLMè¿æ¥æˆåŠŸ! å“åº”: {response}")
            return True
            
    except Exception as e:
        print(f"âŒ LiteLLMè¿æ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_proxy_connectivity():
    """æµ‹è¯•4: æµ‹è¯•ä»£ç†è¿æ¥æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: æµ‹è¯•ä»£ç†å’Œç½‘ç»œè¿æ¥")
    print("=" * 60)
    
    import subprocess
    
    # æµ‹è¯•ä»£ç†æ˜¯å¦å¯ç”¨
    print("\næ£€æŸ¥ä»£ç†æœåŠ¡å™¨...")
    proxy = "http://httpproxy.glm.ai:8888"
    try:
        result = subprocess.run(
            ["curl", "-x", proxy, "-I", "https://openrouter.ai", "-m", "10"],
            capture_output=True,
            text=True,
            timeout=15
        )
        if result.returncode == 0:
            print(f"âœ… ä»£ç†å¯ç”¨ï¼Œå¯ä»¥è®¿é—®OpenRouter")
            print(f"å“åº”å¤´: {result.stdout[:200]}")
            return True
        else:
            print(f"âŒ ä»£ç†è¿æ¥å¤±è´¥")
            print(f"é”™è¯¯: {result.stderr}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•æµ‹è¯•ä»£ç†: {e}")
        return False


async def main():
    print("\n" + "=" * 70)
    print("Adaptive Rubricç”Ÿæˆæµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"  ä»£ç†: {os.environ.get('http_proxy')}")
    print(f"  API Base: {os.environ.get('OPENAI_API_BASE')}")
    print(f"  æ¨¡å‹: {os.environ.get('RUBRIC_GENERATION_MODEL')}")
    print("=" * 70)
    
    results = {}
    
    # æŒ‰é¡ºåºæ‰§è¡Œæµ‹è¯•
    print("\nå¼€å§‹æµ‹è¯•...\n")
    
    results["proxy"] = await test_proxy_connectivity()
    results["litellm"] = await test_litellm_direct()
    results["simple_rubric"] = await test_rubric_generation_simple()
    results["rubric_with_existing"] = await test_rubric_generation_with_existing()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {test_name:30s}: {status}")
    
    passed_count = sum(results.values())
    total_count = len(results)
    
    print(f"\né€šè¿‡ç‡: {passed_count}/{total_count}")
    
    if passed_count == total_count:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! adaptive rubricåŠŸèƒ½æ­£å¸¸ã€‚")
    elif results.get("proxy") and results.get("litellm"):
        print("\nâš ï¸  åŸºç¡€è¿æ¥æ­£å¸¸ï¼Œä½†rubricç”Ÿæˆå¤±è´¥ã€‚")
        print("å¯èƒ½åŸå› :")
        print("  - Promptå¤ªé•¿å¯¼è‡´è¶…æ—¶")
        print("  - JSONè§£æå¤±è´¥")
        print("  - æ¨¡å‹å“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
    else:
        print("\nâŒ è¿æ¥æµ‹è¯•å¤±è´¥!")
        print("\nè¯Šæ–­å»ºè®®:")
        if not results.get("proxy"):
            print("  1. æ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦å¯ç”¨")
            print("     å‘½ä»¤: curl -x http://httpproxy.glm.ai:8888 https://openrouter.ai")
        if not results.get("litellm"):
            print("  2. éªŒè¯OpenRouter API key")
            print("  3. æ£€æŸ¥OPENAI_API_BASEè®¾ç½®")
            print("  4. å°è¯•ä¸ä½¿ç”¨ä»£ç†ï¼ˆæ³¨é‡Šæ‰proxyè®¾ç½®ï¼‰")
        
        print("\nå¯ä»¥å°è¯•çš„ä¿®å¤:")
        print("  - åœ¨train_dr_tulu.shä¸­æ³¨é‡Šæ‰proxyè®¾ç½®ï¼Œç›´æ¥è¿æ¥")
        print("  - æ›´æ¢API key")
        print("  - ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼ˆå¦‚gpt-4o-miniï¼‰")


if __name__ == "__main__":
    asyncio.run(main())

