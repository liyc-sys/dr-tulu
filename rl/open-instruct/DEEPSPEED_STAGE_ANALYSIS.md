# DeepSpeed Stage选择分析（使用DoRA时）

## DeepSpeed ZeRO各Stage的区别

### Stage 0 (不使用ZeRO)
- **分片内容**: 无
- **每个GPU**: 完整模型 + 完整优化器状态 + 完整梯度
- **通信开销**: 最小（只有梯度同步）
- **适用场景**: 单卡内存充足时

### Stage 1 (优化器状态分片)
- **分片内容**: 优化器状态（Adam的momentum和variance）
- **每个GPU**: 完整模型 + 完整梯度 + 1/N的优化器状态
- **通信开销**: 中等（需要allgather优化器状态）
- **内存节省**: 优化器状态减少到1/N

### Stage 2 (优化器状态 + 梯度分片)
- **分片内容**: 优化器状态 + 梯度
- **每个GPU**: 完整模型 + 1/N的梯度 + 1/N的优化器状态
- **通信开销**: 较高（需要reduce-scatter和allgather）
- **内存节省**: 优化器状态和梯度都减少到1/N

### Stage 3 (优化器状态 + 梯度 + 参数分片)
- **分片内容**: 优化器状态 + 梯度 + 模型参数
- **每个GPU**: 1/N的模型参数 + 1/N的梯度 + 1/N的优化器状态
- **通信开销**: 最高（需要频繁的allgather参数）
- **内存节省**: 所有内容都减少到1/N

## 使用DoRA后的内存分析（7B模型）

### 关键数据
- **总参数**: 7B (14GB in bf16)
- **可训练参数** (lora_r=16): ~7M (0.1% of total)
- **冻结参数**: ~6.993B (99.9% of total)

### 各Stage的内存需求（单卡）

#### Stage 0 (不使用ZeRO)
```
模型参数 (完整):          14 GB (所有参数，包括冻结的)
优化器状态 (仅adapter):   0.056 GB (7M × 8 bytes)
梯度 (仅adapter):         0.014 GB (7M × 2 bytes)
激活值:                   10-20 GB
─────────────────────────
总计:                    ~24-34 GB
```

**优点**:
- ✅ 最快（无通信开销）
- ✅ 最简单（无分片逻辑）
- ✅ 单卡A100 80GB完全够用

**缺点**:
- ⚠️ 需要完整模型在内存中（包括冻结参数）

#### Stage 1 (优化器状态分片)
```
模型参数 (完整):          14 GB
优化器状态 (分片):        0.056 GB / N (N=GPU数)
梯度 (完整):              0.014 GB
激活值:                   10-20 GB
─────────────────────────
单卡总计:                ~24-34 GB (几乎无变化)
```

**分析**: 
- ❌ **不推荐** - DoRA的优化器状态已经很小，分片收益微乎其微
- ❌ 反而增加了通信开销

#### Stage 2 (优化器状态 + 梯度分片)
```
模型参数 (完整):          14 GB
优化器状态 (分片):        0.056 GB / N
梯度 (分片):              0.014 GB / N
激活值:                   10-20 GB
─────────────────────────
单卡总计:                ~24-34 GB (几乎无变化)
```

**分析**:
- ❌ **不推荐** - 可训练参数太少，分片收益极小
- ❌ 通信开销增加，但内存节省可忽略

#### Stage 3 (参数分片)
```
模型参数 (分片):          14 GB / N
优化器状态 (分片):        0.056 GB / N
梯度 (分片):              0.014 GB / N
激活值:                   10-20 GB
─────────────────────────
单卡总计:                ~(14/N + 24) GB
```

**2卡情况**: ~(7 + 24) = 31 GB/卡
**4卡情况**: ~(3.5 + 24) = 27.5 GB/卡

**优点**:
- ✅ 多卡时可以大幅减少单卡内存
- ✅ 可以支持更多卡训练更大模型

**缺点**:
- ⚠️ 通信开销最大（需要频繁allgather参数）
- ⚠️ 训练速度可能变慢（特别是单卡时）

## 推荐方案对比

### 场景1: 单卡 A100 80GB + DoRA

#### 推荐: **Stage 0** ⭐⭐⭐⭐⭐

```yaml
deepspeed_stage: 0
use_peft: true
use_dora: true
lora_r: 16
```

**理由**:
- ✅ 内存需求 ~24-34GB，80GB完全够用
- ✅ **训练速度最快**（无通信开销）
- ✅ 配置最简单
- ✅ 单卡时Stage 3反而会降低速度（参数allgather开销）

**内存使用**:
- 模型参数: 14GB
- 优化器+梯度: 0.07GB (可忽略)
- 激活值: 10-20GB
- **总计: ~24-34GB** (远小于80GB)

### 场景2: 单卡 RTX 3090 24GB + DoRA

#### 推荐: **Stage 0** (如果够用) 或 **Stage 3** (如果不够)

**先尝试Stage 0**:
```yaml
deepspeed_stage: 0
use_peft: true
use_dora: true
lora_r: 8  # 降低rank
per_device_train_batch_size: 1
```

如果Stage 0 OOM，再试Stage 3:
```yaml
deepspeed_stage: 3
use_peft: true
use_dora: true
lora_r: 8
```

**Stage 3在单卡时的问题**:
- ⚠️ 参数分片到1个GPU = 无意义（还是14GB）
- ⚠️ 反而增加了通信开销（虽然单卡时开销很小）
- ⚠️ 代码复杂度增加

**实际上，单卡Stage 3 ≈ Stage 0**，但Stage 0更简单更快。

### 场景3: 多卡训练 (2-4卡)

#### 推荐: **Stage 3** ⭐⭐⭐⭐

```yaml
deepspeed_stage: 3
use_peft: true
use_dora: true
world_size: 2  # 或 4
```

**理由**:
- ✅ 参数分片到多卡，单卡内存减少
- ✅ 可以支持更大的batch size
- ✅ 多卡时通信开销相对可接受

**2卡内存**:
- 单卡: ~(14/2 + 24) = 31GB
- 可以支持更大的激活值缓存

## 性能对比（使用DoRA）

| Stage | 单卡A100 80GB | 单卡RTX 3090 | 2卡A100 40GB | 训练速度 | 推荐度 |
|-------|--------------|--------------|--------------|---------|--------|
| Stage 0 | ✅ 充足 | ⚠️ 可能紧张 | ✅ 充足 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| Stage 1 | ✅ 充足 | ⚠️ 可能紧张 | ✅ 充足 | ⭐⭐⭐⭐ | ⭐⭐ |
| Stage 2 | ✅ 充足 | ⚠️ 可能紧张 | ✅ 充足 | ⭐⭐⭐ | ⭐⭐ |
| Stage 3 | ✅ 充足 | ✅ 可行 | ✅ 充足 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |

## 关键发现

### 1. **使用DoRA后，Stage 1和Stage 2几乎无意义**

原因：
- 可训练参数只有0.1%（7M vs 7B）
- 优化器状态和梯度已经很小（<0.1GB）
- 分片这些内容节省的内存可忽略
- 反而增加了通信开销

### 2. **单卡时，Stage 0通常是最佳选择**

原因：
- 内存需求已经大幅降低（DoRA的功劳）
- Stage 0训练速度最快
- 配置最简单
- Stage 3在单卡时无额外收益

### 3. **多卡时，Stage 3才有意义**

原因：
- 参数分片可以真正减少单卡内存
- 可以支持更大的batch size
- 多卡时通信开销相对可接受

## 实际建议

### 对于7B模型 + DoRA：

1. **单卡A100 80GB**: 
   - **推荐Stage 0** ✅
   - 内存充足，速度最快

2. **单卡RTX 3090 24GB**:
   - **先试Stage 0** ✅
   - 如果OOM，降低lora_r到8，减小batch size
   - 如果还是OOM，考虑Stage 3（但收益有限）

3. **2-4卡训练**:
   - **推荐Stage 3** ✅
   - 参数分片可以支持更大batch size

4. **不使用DoRA**:
   - **必须Stage 3** ✅
   - 否则内存不够

## 总结

**使用DoRA后，不一定需要Stage 3！**

- **单卡 + 内存充足**: Stage 0最佳（最快）
- **单卡 + 内存紧张**: 先试Stage 0，不行再Stage 3
- **多卡训练**: Stage 3有意义（参数分片）

**关键点**: DoRA已经大幅减少了可训练参数，优化器状态和梯度分片（Stage 1/2）的收益微乎其微，只有参数分片（Stage 3）在多卡时才有意义。

