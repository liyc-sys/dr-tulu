#!/usr/bin/env python3
"""
ç®€å•çš„æµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯ DoRA ç›¸å…³çš„å¯¼å…¥å’Œé…ç½®æ˜¯å¦æ­£ç¡®
"""

import sys

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å¿…è¦çš„å¯¼å…¥"""
    print("æµ‹è¯•å¯¼å…¥...")
    
    try:
        from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
        print("âœ… PEFT å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ PEFT å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from open_instruct.model_utils import ModelConfig
        print("âœ… ModelConfig å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ ModelConfig å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True

def test_model_config():
    """æµ‹è¯• ModelConfig æ˜¯å¦åŒ…å« DoRA å‚æ•°"""
    print("\næµ‹è¯• ModelConfig å‚æ•°...")
    
    try:
        from open_instruct.model_utils import ModelConfig
        
        # åˆ›å»ºä¸€ä¸ªé»˜è®¤é…ç½®
        config = ModelConfig()
        
        # æ£€æŸ¥ PEFT/DoRA å‚æ•°
        required_attrs = [
            'use_peft', 'use_dora', 'lora_r', 'lora_alpha', 
            'lora_dropout', 'lora_target_modules'
        ]
        
        for attr in required_attrs:
            if hasattr(config, attr):
                print(f"âœ… {attr}: {getattr(config, attr)}")
            else:
                print(f"âŒ ç¼ºå°‘å‚æ•°: {attr}")
                return False
        
        return True
    except Exception as e:
        print(f"âŒ ModelConfig æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_lora_config():
    """æµ‹è¯• LoraConfig æ˜¯å¦æ”¯æŒ use_dora å‚æ•°"""
    print("\næµ‹è¯• LoraConfig DoRA æ”¯æŒ...")
    
    try:
        from peft import LoraConfig, TaskType
        
        # å°è¯•åˆ›å»ºä¸€ä¸ªå¸¦ DoRA çš„é…ç½®
        config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules=["q_proj", "v_proj"],
            use_dora=True,
        )
        
        print(f"âœ… LoraConfig åˆ›å»ºæˆåŠŸï¼Œuse_dora={config.use_dora}")
        return True
    except Exception as e:
        print(f"âŒ LoraConfig æµ‹è¯•å¤±è´¥: {e}")
        print("æç¤º: è¯·ç¡®ä¿ PEFT ç‰ˆæœ¬ >= 0.13.2")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("DoRA å®ç°éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("å¯¼å…¥æµ‹è¯•", test_imports),
        ("ModelConfig æµ‹è¯•", test_model_config),
        ("LoraConfig æµ‹è¯•", test_lora_config),
    ]
    
    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\nâŒ {name} å‡ºç°å¼‚å¸¸: {e}")
            results.append((name, False))
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    all_passed = all(result for _, result in results)
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DoRA å®ç°æ­£ç¡®ã€‚")
        print("å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯ç”¨ DoRA è®­ç»ƒï¼š")
        print("  --use_peft --use_dora --lora_r 16 --lora_alpha 32 --lora_dropout 0.05")
        return 0
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        return 1

if __name__ == "__main__":
    sys.exit(main())

