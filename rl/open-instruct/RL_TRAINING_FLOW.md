# RLè®­ç»ƒæµç¨‹è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†DR-Tulué¡¹ç›®ä¸­RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰è®­ç»ƒçš„æ•´ä½“æµç¨‹ï¼Œä¸»è¦åŸºäºPPOå’ŒGRPOç®—æ³•ã€‚

## ä¸€ã€è®­ç»ƒå…¥å£

### 1.1 ä¸»å…¥å£å‡½æ•°
è®­ç»ƒä» `main()` å‡½æ•°å¼€å§‹ï¼Œä½äºï¼š
- **PPOè®­ç»ƒ**: `ppo_vllm_thread_ray_gtrl.py`
- **GRPOè®­ç»ƒ**: `grpo_vllm_thread_ray_gtrl.py`

### 1.2 å¯åŠ¨è„šæœ¬
é€šè¿‡ `train_dr_tulu.sh` å¯åŠ¨è®­ç»ƒï¼Œä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š
- æ¨¡å‹è·¯å¾„ã€æ•°æ®é›†é…ç½®
- DeepSpeedé…ç½®ï¼ˆstage 3ï¼‰
- vLLMå¼•æ“é…ç½®
- å¥–åŠ±å‡½æ•°é…ç½®

## äºŒã€åˆå§‹åŒ–é˜¶æ®µ

### 2.1 åŸºç¡€è®¾ç½®
```1714:1731:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
def main(args: Args, tc: TokenizerConfig, model_config: ModelConfig):
    # ------------------------------------------------------------
    # Setup tokenizer
    tc.tokenizer_revision = model_config.model_revision if tc.tokenizer_revision is None else tc.tokenizer_revision
    tc.tokenizer_name_or_path = (
        model_config.model_name_or_path if tc.tokenizer_name_or_path is None else tc.tokenizer_name_or_path
    )
    if (
        tc.tokenizer_revision != model_config.model_revision
        and tc.tokenizer_name_or_path != model_config.model_name_or_path
    ):
        # Warn user if tokenizer and model use different revisions; this is an unusual
        # use case.
        warning = f"""Requested tokenizer revision `{tc.tokenizer_revision=}` is different
                   from the model revision `{model_config.model_revision=}` or the tokenizer name `{tc.tokenizer_name_or_path=}`
                   is different from the model name `{model_config.model_name_or_path=}`."""
        print(warning)
    tokenizer = tc.tokenizer
```

### 2.2 æ•°æ®é›†åŠ è½½
```1800:1836:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
    # ------------------------------------------------------------
    # Set up datasets
    transform_fn_args = [
        {},
        {
            "max_token_length": args.max_token_length,
            "max_prompt_token_length": args.max_prompt_token_length,
        },
    ]
    train_dataset = get_cached_dataset_tulu(
        dataset_mixer_list=args.dataset_mixer_list,
        dataset_mixer_list_splits=args.dataset_mixer_list_splits,
        tc=tc,
        dataset_transform_fn=args.dataset_transform_fn,
        transform_fn_args=transform_fn_args,
        dataset_cache_mode=args.dataset_cache_mode,
        dataset_config_hash=args.dataset_config_hash,
        hf_entity=args.hf_entity,
        dataset_local_cache_dir=args.dataset_local_cache_dir,
        dataset_skip_cache=args.dataset_skip_cache,
    )
    train_dataset = train_dataset.shuffle(seed=args.seed)
    eval_dataset = None
    if len(args.dataset_mixer_eval_list) > 0:
        eval_dataset = get_cached_dataset_tulu(
            args.dataset_mixer_eval_list,
            args.dataset_mixer_eval_list_splits,
            tc,
            args.dataset_transform_fn,
            transform_fn_args,
            hf_entity=args.hf_entity,
            dataset_cache_mode=args.dataset_cache_mode,
            dataset_config_hash=args.dataset_config_eval_hash,
            dataset_local_cache_dir=args.dataset_local_cache_dir,
            dataset_skip_cache=args.dataset_skip_cache,
        )
        eval_dataset = eval_dataset.shuffle(seed=args.seed)
```

### 2.3 æ¨¡å‹å’Œå¼•æ“åˆå§‹åŒ–
```1846:1881:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
    # create the model and optimizer
    pg = None
    bundles = [{"GPU": actor_num_gpus, "CPU": actor_num_gpus * 10} for actor_num_gpus in args.actor_num_gpus_per_node]
    pg = placement_group(bundles, strategy="STRICT_SPREAD")
    ray.get(pg.ready())

    inits = []
    policy_group = ModelGroup(
        pg,
        PolicyTrainerRayProcess,
        args.actor_num_gpus_per_node,
        args.single_gpu_mode,
    )
    wandb_url = wandb.run.get_url() if args.with_tracking else None
    inits.extend(
        model.from_pretrained.remote(args, model_config, beaker_config, wandb_url) for model in policy_group.models
    )
    max_len = args.max_prompt_token_length + args.response_length
    vllm_engines = create_vllm_engines(
        args.vllm_num_engines,
        args.vllm_tensor_parallel_size,
        args.vllm_enforce_eager,
        tc.tokenizer_name_or_path,
        model_config.model_name_or_path,
        model_config.model_revision,
        args.seed,
        args.enable_prefix_caching,
        max_len,
        args.vllm_gpu_memory_utilization,
        args.single_gpu_mode,
        pg=pg if args.single_gpu_mode else None,
    )

    metrics_queue = RayQueue()
    ray.get(inits)
    print("======== all models initialized =========")
```

### 2.4 å¯åŠ¨è®­ç»ƒè¿›ç¨‹
```1883:1894:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
    refs = []
    for i, policy_model in enumerate(policy_group.models):
        refs.append(
            policy_model.train.remote(
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=tokenizer,
                vllm_engines=vllm_engines,
                metrics_queue=metrics_queue,
                data_collator=data_collator,
            )
        )
```

## ä¸‰ã€è®­ç»ƒå¾ªç¯ï¼ˆæ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼‰

è®­ç»ƒå¾ªç¯åœ¨ `PolicyTrainerRayProcess.train()` æ–¹æ³•ä¸­æ‰§è¡Œï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹é˜¶æ®µï¼š

### 3.1 Rollouté˜¶æ®µï¼šç”Ÿæˆå“åº”

#### 3.1.1 å¼‚æ­¥ç”Ÿæˆçº¿ç¨‹
ä½¿ç”¨vLLMå¼•æ“åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­å¼‚æ­¥ç”Ÿæˆå“åº”ï¼š

```1034:1080:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
        def vllm_generate(
            generation_config: SamplingParams,
            response_ids_Q: Queue,
            param_prompt_Q: Queue,
            num_training_steps: int,
            sample_evaluation_prompt_token_ids: Optional[List[int]],
            evaluation_Q: Queue,
            eval_freq: int,
            resume_training_step: int,
        ):
            def generate_with_engines(prompts: List[List[int]], sampling_params: SamplingParams):
                # Split queries between engines
                queries_per_engine = math.ceil(len(prompts) / len(vllm_engines))
                split_queries = [
                    prompts[i : i + queries_per_engine] for i in range(0, len(prompts), queries_per_engine)
                ]
                # Generate responses in parallel across engines
                futures = [
                    vllm_engine.generate.remote(
                        sampling_params=sampling_params, prompt_token_ids=queries, use_tqdm=False
                    )
                    for vllm_engine, queries in zip(vllm_engines, split_queries)
                ]
                # Gather all responses
                all_outputs = ray.get(futures)
                response_ids = []
                for outputs in all_outputs:
                    response_ids.extend([list(out.token_ids) for output in outputs for out in output.outputs])
                return response_ids

            for training_step in range(resume_training_step, num_training_steps + 1):
                items = param_prompt_Q.get()
                if items is None:
                    break
                _, g_queries_list = items

                with Timer("ğŸ”¥ğŸ”¥ğŸ”¥ Generation time", noop=self.rank != 0):
                    response_ids = generate_with_engines(g_queries_list, generation_config)
                response_ids_Q.put(response_ids)

                # Evaluate the model
                if sample_evaluation_prompt_token_ids is not None and (training_step - 1) % eval_freq == 0:
                    response_ids = generate_with_engines(
                        sample_evaluation_prompt_token_ids, evaluation_generation_config
                    )
                    evaluation_Q.put(response_ids)
```

#### 3.1.2 è·å–ç”Ÿæˆçš„å“åº”
```1219:1234:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
                if self.rank == 0:
                    g_response_token_ids = response_ids_Q.get()
                    DUMMY_PAD_TOKEN = (
                        args.stop_token_id
                    )  # we can't use tokenizer.pad_token_id because it's outside vocab and `torch.gather(all_logprob, 2, response.unsqueeze(-1))` will error out
                    g_padded_response_ids = [
                        response + [DUMMY_PAD_TOKEN] * (args.response_length - len(response))
                        for response in g_response_token_ids
                    ]
                    g_padded_response_ids = torch.tensor(g_padded_response_ids, device=device)
                    g_vllm_responses[:] = g_padded_response_ids
                dist.broadcast(g_vllm_responses, src=0)
                local_vllm_responses = g_vllm_responses[
                    accelerator.process_index * queries.shape[0] : (accelerator.process_index + 1) * queries.shape[0]
                ]
                query_responses = torch.cat((queries, local_vllm_responses), 1)
```

### 3.2 å¥–åŠ±è®¡ç®—é˜¶æ®µ

#### 3.2.1 è®¡ç®—ç­–ç•¥å’Œå‚è€ƒæ¨¡å‹çš„logprobs
```1246:1261:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
                    # Get policy model logprob
                    logprob = self.forward(
                        self.model, query_response, response, tokenizer.pad_token_id, context_length, args.temperature
                    )
                    torch.cuda.empty_cache()

                    # Get reference model logprob
                    ref_logprob = self.forward(
                        self.ref_policy,
                        query_response,
                        response,
                        tokenizer.pad_token_id,
                        context_length,
                        args.temperature,
                    )
                    torch.cuda.empty_cache()
```

#### 3.2.2 å¤„ç†å“åº”å¹¶è®¡ç®—å¥–åŠ±
```1263:1302:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
                    # Response Processing 1. truncate response after the first occurrence of `stop_token_id`
                    postprocessed_response = response
                    if args.stop_token_id is not None:  # handle the edge case when stop_token_id exists but is 0
                        postprocessed_response = truncate_response(
                            args.stop_token_id, tokenizer.pad_token_id, response
                        )
                    # Response Processing 2. run reward model on the truncated responses
                    postprocessed_query_response = torch.cat((query, postprocessed_response), 1)
                    sequence_length = first_true_indices(postprocessed_response == tokenizer.pad_token_id) - 1
                    score = torch.zeros(query.shape[0], device=query.device)
                    if args.reward_model_multiplier:
                        _, score, _ = get_reward(
                            self.reward_model, postprocessed_query_response, tokenizer.pad_token_id, context_length
                        )
                        score *= args.reward_model_multiplier
                    if args.apply_verifiable_reward:
                        # we need to batch the gt to match query.
                        ground_truth = ground_truths[i : i + args.local_rollout_forward_batch_size]
                        dataset = datasets[i : i + args.local_rollout_forward_batch_size]
                        decoded_response = tokenizer.batch_decode(postprocessed_response)
                        # for now, not supporting arb log values in non-fast scripts.
                        verifiable_reward, per_func_reward, _ = apply_verifiable_reward(
                            responses=postprocessed_response,
                            decoded_responses=decoded_response,
                            ground_truths=ground_truth,
                            datasets=dataset,
                            reward_mult=args.verification_reward,
                        )
                        verifiable_reward = torch.tensor(verifiable_reward, device=score.device)
                        verifiable_count = verifiable_reward > 0
                        score += verifiable_reward
                        # For each sample, aggregate each per-function reward into a single dict.
                        for reward_dict in per_func_reward:
                            for key, value in reward_dict.items():
                                per_func_rewards[key].append(value)
                    else:
                        verifiable_count = torch.tensor([0.0], device=device).float()

                    if args.add_r1_style_format_reward:
                        score += format_scores[i : i + args.local_rollout_forward_batch_size]

                    full_value, _, _ = get_reward(
                        self.value_model, query_response, tokenizer.pad_token_id, context_length
                    )
                    value = full_value[:, context_length - 1 : -1].squeeze(-1)
```

#### 3.2.3 è®¡ç®—KLæ•£åº¦å’Œå¥–åŠ±
```1352:1388:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
                # 4. compute rewards
                kl1 = logprobs - ref_logprobs
                kl2 = (kl1) ** 2 / 2
                kl3 = (-kl1).exp() - 1 + kl1
                if args.kl_estimator == "kl1":
                    kl = kl1
                elif args.kl_estimator == "kl2":
                    kl = kl2
                elif args.kl_estimator == "kl3":
                    kl = kl3
                non_score_reward = -args.beta * kl
                non_score_reward_sum = non_score_reward.sum(1)
                rlhf_reward = scores + non_score_reward_sum
                rewards = non_score_reward.clone()
                actual_start = torch.arange(rewards.size(0), device=rewards.device)
                actual_end = torch.where(sequence_lengths_p1 < rewards.size(1), sequence_lengths_p1, sequence_lengths)
                rewards[[actual_start, actual_end]] += scores

                # 5. whiten rewards
                if args.whiten_rewards:
                    rewards = masked_whiten(rewards, mask=~padding_mask_p1, shift_mean=False)
                    rewards = torch.masked_fill(rewards, padding_mask_p1, 0)

                # 6. compute advantages and returns
                lastgaelam = 0
                advantages_reversed = []
                gen_length = responses.shape[1]
                for t in reversed(range(gen_length)):
                    nextvalues = values[:, t + 1] if t < gen_length - 1 else 0.0
                    delta = rewards[:, t] + args.gamma * nextvalues - values[:, t]
                    lastgaelam = delta + args.gamma * args.lam * lastgaelam
                    advantages_reversed.append(lastgaelam)
                advantages = torch.stack(advantages_reversed[::-1], axis=1)
                returns = advantages + values
                advantages = masked_whiten(advantages, ~padding_mask)
                advantages = torch.masked_fill(advantages, padding_mask, 0)
                torch.cuda.empty_cache()
```

### 3.3 ç­–ç•¥æ›´æ–°é˜¶æ®µï¼ˆPPOï¼‰

#### 3.3.1 å¤šè½®è®­ç»ƒ
```1390:1464:rl/open-instruct/open_instruct/ppo_vllm_thread_ray_gtrl.py
            # Do multiple epochs of training on on-policy data (PPO-style), with a fresh random shuffle in each epoch
            for epoch_idx in range(args.num_epochs):
                b_inds = np.random.permutation(args.local_total_prompts)
                minibatch_idx = 0
                for mini_batch_start in range(0, args.local_total_prompts, args.local_mini_batch_size):
                    mini_batch_end = mini_batch_start + args.local_mini_batch_size
                    mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]
                    gradient_accumulation_idx = 0
                    # NOTE: deepspeed handles gradient accumulation automatically; see https://github.com/microsoft/DeepSpeed/issues/758#issuecomment-801580724
                    for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):
                        # print("micro batch start", micro_batch_start, self.rank)
                        micro_batch_end = micro_batch_start + args.per_device_train_batch_size
                        micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]
                        mb_advantage = advantages[micro_batch_inds]
                        mb_responses = responses[micro_batch_inds]
                        mb_query_responses = query_responses[micro_batch_inds]
                        mb_logprobs = logprobs[micro_batch_inds]
                        mb_return = returns[micro_batch_inds]
                        mb_values = values[micro_batch_inds]
                        mb_padding_mask_p1 = padding_mask_p1[micro_batch_inds]

                        vpred_temp = get_reward(
                            self.value_model, mb_query_responses, tokenizer.pad_token_id, context_length
                        )
                        vpred_temp = vpred_temp[0]
                        vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)
                        vpred = torch.masked_fill(vpred, mb_padding_mask_p1, 0)
                        vpredclipped = torch.clamp(
                            vpred,
                            mb_values - args.cliprange_value,
                            mb_values + args.cliprange_value,
                        )
                        vf_losses1 = torch.square(vpred - mb_return)
                        vf_losses2 = torch.square(vpredclipped - mb_return)
                        vf_loss_max = torch.max(vf_losses1, vf_losses2)
                        vf_loss = 0.5 * masked_mean(vf_loss_max, ~mb_padding_mask_p1)
                        self.value_model.backward(vf_loss * args.vf_coef)
                        self.value_model.step()

                        new_logprobs = self.forward(
                            self.model,
                            mb_query_responses,
                            mb_responses,
                            tokenizer.pad_token_id,
                            context_length,
                            args.temperature,
                        )
                        new_logprobs = torch.masked_fill(new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB)
                        logprobs_diff = new_logprobs - mb_logprobs
                        ratio = torch.exp(logprobs_diff)
                        pg_losses = -mb_advantage * ratio
                        pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)
                        pg_loss_max = torch.max(pg_losses, pg_losses2)
                        pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])
                        loss = pg_loss
                        self.model.backward(loss)
                        self.model.step()
                        with torch.no_grad():
                            vf_clipfrac = masked_mean((vf_losses2 > vf_losses1).float(), ~mb_padding_mask_p1)
                            pg_clipfrac = masked_mean(
                                (pg_losses2 > pg_losses).float(), ~padding_mask[micro_batch_inds]
                            )
                            # print("value model stepped", self.rank, "micro batch start", micro_batch_start)
                            # prob_dist = torch.nn.functional.softmax(logits, dim=-1)
                            # entropy = torch.logsumexp(logits, dim=-1) - torch.sum(prob_dist * logits, dim=-1)
                            approxkl = 0.5 * (logprobs_diff**2).mean()
                            approxkl_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = approxkl
                            pg_clipfrac_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = pg_clipfrac
                            pg_loss_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = pg_loss
                            vf_loss_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = vf_loss
                            vf_clipfrac_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = vf_clipfrac
                            # entropy_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = entropy.mean()
                            ratio_stats[epoch_idx, minibatch_idx, gradient_accumulation_idx] = ratio.mean()
                        gradient_accumulation_idx += 1
                    minibatch_idx += 1
                    # fmt: off
                    del mb_advantage, mb_responses, mb_query_responses, mb_logprobs, mb_return, mb_values, mb_padding_mask_p1
                    del new_logprobs, logprobs_diff, ratio, pg_losses, pg_losses2, pg_loss_max, pg_loss, loss
                    # fmt: on
                    # del everything and empty cache
                    torch.cuda.empty_cache()
                del b_inds, mini_batch_inds
```

## å››ã€GRPOä¸PPOçš„ä¸»è¦åŒºåˆ«

### 4.1 GRPOçš„ä¼˜åŠ¿
GRPOï¼ˆGroup Relative Policy Optimizationï¼‰ä¸PPOçš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š

1. **ä¸éœ€è¦ä»·å€¼æ¨¡å‹**ï¼šGRPOç›´æ¥ä½¿ç”¨ç»„å†…ç›¸å¯¹å¥–åŠ±ï¼Œä¸éœ€è¦è®­ç»ƒä»·å€¼å‡½æ•°
2. **ä¼˜åŠ¿è®¡ç®—æ–¹å¼ä¸åŒ**ï¼š
   - PPOï¼šä½¿ç”¨GAEï¼ˆGeneralized Advantage Estimationï¼‰è®¡ç®—ä¼˜åŠ¿
   - GRPOï¼šä½¿ç”¨ç»„å†…æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºä¼˜åŠ¿

```1317:1322:rl/open-instruct/open_instruct/grpo_vllm_thread_ray_gtrl.py
                # MAIN GRPO CHANGE: compute group rewards instead of value model output
                mean_grouped_rewards = scores.view(-1, args.number_samples_per_prompt).mean(dim=-1)
                mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(args.number_samples_per_prompt, dim=0)
                std_grouped_rewards = scores.view(-1, args.number_samples_per_prompt).std(dim=-1)
                std_grouped_rewards = std_grouped_rewards.repeat_interleave(args.number_samples_per_prompt, dim=0)
                advantages = (scores - mean_grouped_rewards) / (std_grouped_rewards + 1e-8)
```

3. **æŸå¤±å‡½æ•°**ï¼šGRPOåœ¨æŸå¤±ä¸­ç›´æ¥åŠ å…¥KLæ•£åº¦é¡¹

```1385:1388:rl/open-instruct/open_instruct/grpo_vllm_thread_ray_gtrl.py
                        # grpo change: directly subtract KL in loss (add)
                        loss = masked_mean(pg_loss_max + (args.beta * kl), ~padding_mask[micro_batch_inds])
                        self.model.backward(loss)
                        self.model.step()
```

## äº”ã€å…³é”®ç»„ä»¶è¯´æ˜

### 5.1 æ¨¡å‹ç»„ä»¶
- **ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰**ï¼šæ­£åœ¨è®­ç»ƒçš„ä¸»è¦æ¨¡å‹
- **å‚è€ƒæ¨¡å‹ï¼ˆReference Modelï¼‰**ï¼šç”¨äºè®¡ç®—KLæ•£åº¦çš„å›ºå®šæ¨¡å‹
- **ä»·å€¼æ¨¡å‹ï¼ˆValue Modelï¼‰**ï¼šä»…PPOéœ€è¦ï¼Œç”¨äºä¼°è®¡çŠ¶æ€ä»·å€¼
- **å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰**ï¼šå¯é€‰ï¼Œç”¨äºè®¡ç®—å¥–åŠ±åˆ†æ•°

### 5.2 vLLMå¼•æ“
- ç”¨äºé«˜æ•ˆç”Ÿæˆå“åº”
- æ”¯æŒå¤šå¼•æ“å¹¶è¡Œç”Ÿæˆ
- é€šè¿‡Rayè¿›è¡Œåˆ†å¸ƒå¼ç®¡ç†

### 5.3 å¥–åŠ±å‡½æ•°
- **å¯éªŒè¯å¥–åŠ±ï¼ˆVerifiable Rewardï¼‰**ï¼šåŸºäºground truthçš„å¥–åŠ±
- **å¥–åŠ±æ¨¡å‹å¥–åŠ±**ï¼šåŸºäºè®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹
- **æ ¼å¼å¥–åŠ±**ï¼šåŸºäºå“åº”æ ¼å¼çš„å¥–åŠ±
- **KLæƒ©ç½š**ï¼šé˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ

### 5.4 DeepSpeedé›†æˆ
- ä½¿ç”¨DeepSpeed ZeRO Stage 3è¿›è¡Œå‚æ•°åˆ†ç‰‡
- è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯
- æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒ

## å…­ã€è®­ç»ƒæµç¨‹æ€»ç»“

```
1. åˆå§‹åŒ–
   â”œâ”€â”€ åŠ è½½tokenizerå’Œæ•°æ®é›†
   â”œâ”€â”€ åˆ›å»ºRayè¿›ç¨‹ç»„å’ŒvLLMå¼•æ“
   â”œâ”€â”€ åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹ã€å‚è€ƒæ¨¡å‹ã€ä»·å€¼æ¨¡å‹ï¼ˆPPOï¼‰
   â””â”€â”€ å¯åŠ¨è®­ç»ƒè¿›ç¨‹

2. è®­ç»ƒå¾ªç¯ï¼ˆæ¯ä¸ªtraining stepï¼‰
   â”œâ”€â”€ Rollouté˜¶æ®µ
   â”‚   â”œâ”€â”€ ä»æ•°æ®é›†é‡‡æ ·prompts
   â”‚   â”œâ”€â”€ ä½¿ç”¨vLLMå¼‚æ­¥ç”Ÿæˆå“åº”
   â”‚   â””â”€â”€ å¹¿æ’­å“åº”åˆ°æ‰€æœ‰è¿›ç¨‹
   â”‚
   â”œâ”€â”€ å¥–åŠ±è®¡ç®—é˜¶æ®µ
   â”‚   â”œâ”€â”€ è®¡ç®—ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„logprobs
   â”‚   â”œâ”€â”€ å¤„ç†å“åº”ï¼ˆæˆªæ–­ã€è¿‡æ»¤ï¼‰
   â”‚   â”œâ”€â”€ è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼ˆå¯éªŒè¯å¥–åŠ±ã€å¥–åŠ±æ¨¡å‹ç­‰ï¼‰
   â”‚   â”œâ”€â”€ è®¡ç®—KLæ•£åº¦
   â”‚   â”œâ”€â”€ è®¡ç®—ä¼˜åŠ¿ï¼ˆPPO: GAE, GRPO: ç»„å†…æ ‡å‡†åŒ–ï¼‰
   â”‚   â””â”€â”€ è®¡ç®—returns
   â”‚
   â””â”€â”€ ç­–ç•¥æ›´æ–°é˜¶æ®µ
       â”œâ”€â”€ å¤šè½®è®­ç»ƒï¼ˆnum_epochsï¼‰
       â”œâ”€â”€ å°æ‰¹é‡è®­ç»ƒï¼ˆmini batchesï¼‰
       â”œâ”€â”€ æ¢¯åº¦ç´¯ç§¯ï¼ˆmicro batchesï¼‰
       â”œâ”€â”€ æ›´æ–°ä»·å€¼æ¨¡å‹ï¼ˆä»…PPOï¼‰
       â”œâ”€â”€ æ›´æ–°ç­–ç•¥æ¨¡å‹ï¼ˆPPO/GRPOï¼‰
       â””â”€â”€ è®°å½•æŒ‡æ ‡

3. ä¿å­˜å’Œè¯„ä¼°
   â”œâ”€â”€ å®šæœŸä¿å­˜checkpoint
   â”œâ”€â”€ å®šæœŸè¯„ä¼°æ¨¡å‹
   â””â”€â”€ è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°wandb/tensorboard
```

## ä¸ƒã€å·¥å…·è°ƒç”¨æœºåˆ¶

### 7.1 å¯ç”¨å·¥å…·åˆ—è¡¨

è®­ç»ƒæ—¶LLMå¯ä»¥è°ƒç”¨çš„å·¥å…·é€šè¿‡MCPï¼ˆModel Context Protocolï¼‰åè®®æä¾›ã€‚å·¥å…·æ³¨å†Œè¡¨å®šä¹‰åœ¨ï¼š

```21:27:rl/open-instruct/open_instruct/search_utils/mcp_tools.py
MCP_TOOL_REGISTRY = {
    "snippet_search": SemanticScholarSnippetSearchTool,
    "google_search": SerperSearchTool,
    "massive_serve": MassiveServeSearchTool,
    "browse_webpage": Crawl4AIBrowseTool,
    # "browse_webpage": SerperBrowseTool
}
```

#### 7.1.1 æœç´¢å·¥å…·

1. **snippet_search** (Semantic Scholar Snippet Search)
   - åŠŸèƒ½ï¼šä»å­¦æœ¯è®ºæ–‡ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬ç‰‡æ®µ
   - å®šä¹‰ä½ç½®ï¼š`agent/dr_agent/mcp_backend/main.py` çš„ `semantic_scholar_snippet_search` å‡½æ•°
   - å‚æ•°ï¼š
     - `query`: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
     - `year`: å‘è¡¨å¹´ä»½è¿‡æ»¤ï¼ˆå¦‚ "2021-2025"ï¼‰
     - `limit`: è¿”å›çš„ç‰‡æ®µæ•°é‡
     - `fieldsOfStudy`: ç ”ç©¶é¢†åŸŸè¿‡æ»¤
   - ç”¨é€”ï¼šæŸ¥æ‰¾ç§‘å­¦æ–‡çŒ®ä¸­çš„å…·ä½“å¼•ç”¨å’Œè¯æ®

2. **google_search** (Serper Google Search)
   - åŠŸèƒ½ï¼šé€šç”¨ç½‘é¡µæœç´¢
   - å®šä¹‰ä½ç½®ï¼š`agent/dr_agent/mcp_backend/main.py` çš„ `serper_google_webpage_search` å‡½æ•°
   - å‚æ•°ï¼š
     - `query`: æœç´¢æŸ¥è¯¢
     - `num_results`: è¿”å›ç»“æœæ•°é‡
     - `gl`: åœ°ç†ä½ç½®ä»£ç 
     - `hl`: ç•Œé¢è¯­è¨€
   - ç”¨é€”ï¼šæŸ¥æ‰¾ä¸€èˆ¬ç½‘ç»œä¿¡æ¯å’Œèµ„æº

3. **massive_serve** (Massive Serve Search)
   - åŠŸèƒ½ï¼šä½¿ç”¨å¯†é›†æ®µè½æ£€ç´¢è¿›è¡Œå¤§è§„æ¨¡æ–‡æ¡£æœç´¢
   - å®šä¹‰ä½ç½®ï¼š`agent/dr_agent/mcp_backend/main.py` çš„ `massive_serve_search` å‡½æ•°
   - å‚æ•°ï¼š
     - `query`: æœç´¢æŸ¥è¯¢
     - `n_docs`: è¿”å›æ–‡æ¡£æ•°é‡
     - `domains`: æœç´¢åŸŸ/ç´¢å¼•
   - ç”¨é€”ï¼šè®¿é—®å¤§è§„æ¨¡æ–‡æ¡£é›†åˆ

#### 7.1.2 æµè§ˆå·¥å…·

4. **browse_webpage** (Crawl4AI Browse)
   - åŠŸèƒ½ï¼šè·å–ç½‘é¡µå†…å®¹å¹¶æå–å¯è¯»æ–‡æœ¬
   - å®šä¹‰ä½ç½®ï¼š`agent/dr_agent/mcp_backend/main.py` çš„ `crawl4ai_fetch_webpage_content` å‡½æ•°
   - å‚æ•°ï¼š
     - `url`: è¦è·å–çš„ç½‘é¡µURL
     - `ignore_links`: æ˜¯å¦ç§»é™¤markdownä¸­çš„è¶…é“¾æ¥
     - `use_pruning`: æ˜¯å¦åº”ç”¨å†…å®¹è¿‡æ»¤
     - `bm25_query`: å¯é€‰çš„BM25æŸ¥è¯¢ç”¨äºå†…å®¹è¿‡æ»¤
   - ç”¨é€”ï¼šæ‰“å¼€å¹¶é˜…è¯»ç½‘é¡µçš„å®Œæ•´å†…å®¹

#### 7.1.3 å…¶ä»–å·¥å…·ï¼ˆåœ¨MCPåç«¯å®šä¹‰ä½†å¯èƒ½æœªåœ¨æ³¨å†Œè¡¨ä¸­ï¼‰

- **semantic_scholar_search**: ä½¿ç”¨Semantic Scholar APIæœç´¢å­¦æœ¯è®ºæ–‡
- **pubmed_search**: ä½¿ç”¨PubMed APIæœç´¢åŒ»å­¦å’Œç§‘å­¦è®ºæ–‡
- **serper_google_scholar_search**: ä½¿ç”¨Google Scholaræœç´¢å­¦æœ¯è®ºæ–‡
- **vllm_hosted_reranker**: ä½¿ç”¨VLLMæ‰˜ç®¡çš„rerankerå¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
- **jina_fetch_webpage_content**: ä½¿ç”¨Jina Reader APIè·å–ç½‘é¡µå†…å®¹

### 7.2 å·¥å…·è°ƒç”¨æ ¼å¼

å·¥å…·é€šè¿‡ç»Ÿä¸€çš„XMLæ ‡ç­¾æ ¼å¼è°ƒç”¨ï¼š

```xml
<call_tool name="tool_name">query or parameters</call_tool>
```

ç¤ºä¾‹ï¼š
```xml
<call_tool name="google_search">2024 renewable energy market trends</call_tool>
<call_tool name="snippet_search" limit="8" year="2021-2025" fieldsOfStudy="Computer Science, Medicine">large language model retrieval evaluation</call_tool>
<call_tool name="browse_webpage">https://example.com/article</call_tool>
```

### 7.3 å·¥å…·è¾“å‡ºæ ¼å¼

å·¥å…·æ‰§è¡Œåï¼Œç»“æœä¼šè¢«åŒ…è£…åœ¨ `<tool_output>` æ ‡ç­¾ä¸­ï¼š

```xml
<tool_output>
  <snippet id="UNIQUE_ID">content</snippet>
  <snippet id="UNIQUE_ID2">content</snippet>
</tool_output>
```

å¯¹äºç½‘é¡µæµè§ˆï¼š
```xml
<tool_output>
  <webpage id="UNIQUE_ID">content</webpage>
</tool_output>
```

### 7.4 ç³»ç»Ÿæç¤ºè¯

å·¥å…·çš„ä½¿ç”¨è¯´æ˜å®šä¹‰åœ¨ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶ä¸­ï¼š

- ä¸»è¦æ–‡ä»¶ï¼š`rl/open-instruct/open_instruct/search_utils/system_prompts/unified_tool_calling_v20250907.yaml`
- è¯¥æ–‡ä»¶åŒ…å«ï¼š
  - å·¥å…·è°ƒç”¨æ ¼å¼è¯´æ˜
  - æ¯ä¸ªå·¥å…·çš„ç”¨é€”å’Œå‚æ•°
  - å·¥ä½œæµç¨‹ç¤ºä¾‹
  - å¼•ç”¨æ ¼å¼è¦æ±‚

### 7.5 å·¥å…·é›†æˆåˆ°è®­ç»ƒæµç¨‹

#### 7.5.1 å·¥å…·æ³¨å†Œ

åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼Œå·¥å…·é€šè¿‡ä»¥ä¸‹æ–¹å¼æ³¨å†Œï¼š

```1967:1991:rl/open-instruct/open_instruct/grpo_fast.py
    # first, handle the "regular" tools of search and code via actors.
    if args.tools:
        for tool in args.tools:
            class_path = TOOL_CLASS_REGISTRY.get(tool.lower(), None)
            if class_path is None:
                raise ValueError(f"Unknown tool: {tool}")
            # Pass the entire args namespace; ToolActor will filter valid kwargs
            _register_actor_backed_tool(class_path=class_path, init_kwargs=vars(args))

    vllm_engines = create_vllm_engines(
        args.vllm_num_engines,
        args.vllm_tensor_parallel_size,
        args.vllm_enforce_eager,
        tc.tokenizer_name_or_path,
        model_config.model_name_or_path,
        model_config.model_revision,
        args.seed,
        args.vllm_enable_prefix_caching,
        max_len,
        args.vllm_gpu_memory_utilization,
        args.single_gpu_mode,
        pg=pg if args.single_gpu_mode else None,
        tools=tool_objects,
        max_tool_calls=args.max_tool_calls,
    )
```

#### 7.5.2 MCPå·¥å…·åŒ…è£…å™¨

MCPå·¥å…·é€šè¿‡ `MCPTool` ç±»åŒ…è£…ï¼š

```57:133:rl/open-instruct/open_instruct/search_utils/mcp_tools.py
class MCPTool(Tool):
    """
    Unlike other tools, this guy handles *all mcp tools*. Why?
    because they share the same end string (</tool>). Hence, we need the parsers
    to work out how to route them. Ideally, this would be more tightly integrated into vllm,
    but for now, this is a bit cleaner.
    """
    def __init__(
        self,
        mcp_tool_names: List[str] | str,
        mcp_parser_name: str = "unified",
        transport_type: str | None = None,
        mcp_host: str | None = None,
        mcp_port: int | None = None,
        max_retries: int = 3,
        retry_backoff: float = 0.5,
        search_api_endpoint: str | None = None,
        start_str: str = "",
        end_str: str | None = None,
        mcp_timeout: int = 180,
        base_url: str | None = None,
        number_documents_to_search: int = 10,
        use_localized_snippets: bool = False,
        context_chars: int = 6000,
        *args,
        **kwargs,
    ):
        self.mcp_tools = []
        self.stop_strings = []
        # Allow selecting transport via arg or env; default to StreamableHttpTransport
        self.transport_type = transport_type or os.environ.get("MCP_TRANSPORT", "StreamableHttpTransport")
        self.mcp_host = mcp_host or os.environ.get("MCP_TRANSPORT_HOST", "0.0.0.0")
        if self.mcp_host is not None:
            os.environ["MCP_TRANSPORT_HOST"] = str(self.mcp_host)
        self.mcp_port = mcp_port or os.environ.get("MCP_TRANSPORT_PORT", 8000)
        if self.mcp_port is not None:
            os.environ["MCP_TRANSPORT_PORT"] = str(self.mcp_port)
        # Transient error retry settings
        self.max_retries = max_retries
        self.retry_backoff = retry_backoff
        # Support comma-separated string for mcp_tool_names
        if isinstance(mcp_tool_names, str):
            mcp_tool_names = [n.strip() for n in mcp_tool_names.split(",") if n.strip()]
        for mcp_tool_name in mcp_tool_names:
            # filter kwargs so we only pass ones the constructor understands
            mcp_tool_cls = MCP_TOOL_REGISTRY[mcp_tool_name]
            sig = inspect.signature(mcp_tool_cls.__init__)
            valid_params = set(sig.parameters.keys())
            filtered_kwargs = {
                k: v for k, v in kwargs.items() if k in valid_params
            }
            if "base_url" in valid_params:
                filtered_kwargs["base_url"] = base_url
            if "number_documents_to_search" in valid_params:
                filtered_kwargs["number_documents_to_search"] = number_documents_to_search
            if "use_localized_snippets" in valid_params:
                filtered_kwargs["use_localized_snippets"] = use_localized_snippets
            if "context_chars" in valid_params:
                filtered_kwargs["context_chars"] = context_chars
            # special case for crawl4ai
            if mcp_tool_name == "browse_webpage":
                filtered_kwargs["use_docker_version"] = True
                filtered_kwargs["use_ai2_config"] = True
            # basically, we want to defer as much as possible to the mcp tool.
            # this 'tool' actually just passes everything down to the mcp tool.
            self.mcp_tools.append(mcp_tool_cls(
                timeout=mcp_timeout,
                name=mcp_tool_name,
                tool_parser=mcp_parser_name,
                transport_type=self.transport_type,
                **filtered_kwargs,
            ))
            # assign the stop strings from the parser itself.
            self.stop_strings += self.mcp_tools[-1].tool_parser.stop_sequences
        # MCP tool handles its own start and end strings.
        super().__init__(start_str=start_str, end_str=end_str or self.stop_strings[-1])
```

#### 7.5.3 vLLMå·¥å…·é›†æˆ

å·¥å…·é€šè¿‡ `ToolUseLLM` ç±»é›†æˆåˆ°vLLMä¸­ï¼š

```151:159:rl/open-instruct/open_instruct/tool_utils/tool_vllm.py
class ToolUseLLM(LLM):
    def __init__(self, tools: dict[str, Tool] = None, max_tool_calls: Union[int, dict[str, int]] = 4, *args, **kwargs):
        
        # Convert max_tool_calls to a dict if it's an int
        if isinstance(max_tool_calls, int):
            self.max_tool_calls = {k: max_tool_calls for k in tools.keys()} if tools else {}
        else:
            self.max_tool_calls = max_tool_calls
        # Initialize executor and store for pending tool calls
        self.executor = ThreadPoolExecutor(max_workers=20)
        self.pending_tool_futures = {}
```

### 7.6 è®­ç»ƒè„šæœ¬ä¸­çš„å·¥å…·é…ç½®

åœ¨ `train_dr_tulu.sh` ä¸­ï¼Œå·¥å…·é€šè¿‡ä»¥ä¸‹å‚æ•°é…ç½®ï¼š

```bash
--tools mcp \
--mcp_tool_names 'snippet_search,google_search,browse_webpage' \
--max_tool_calls 10 \
--system_prompt_file open_instruct/search_utils/system_prompts/unified_tool_calling_v20250907.yaml \
--mcp_parser_name v20250824 \
--mcp_server_command "'python -m dr_agent.mcp_backend.main --transport http --port 8003 --host 0.0.0.0 --path /mcp'"
```

### 7.7 MCPæœåŠ¡å™¨

MCPå·¥å…·é€šè¿‡ç‹¬ç«‹çš„MCPæœåŠ¡å™¨æä¾›ï¼ŒæœåŠ¡å™¨å®šä¹‰åœ¨ï¼š
- `agent/dr_agent/mcp_backend/main.py`

æœåŠ¡å™¨ä½¿ç”¨FastMCPæ¡†æ¶ï¼Œé€šè¿‡HTTPä¼ è¾“åè®®æä¾›æœåŠ¡ã€‚è®­ç»ƒæ—¶ä¼šå¯åŠ¨MCPæœåŠ¡å™¨å­è¿›ç¨‹æ¥å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚ã€‚

### 7.8 å·¥å…·è°ƒç”¨æµç¨‹

1. **ç”Ÿæˆé˜¶æ®µ**ï¼šLLMç”ŸæˆåŒ…å«å·¥å…·è°ƒç”¨æ ‡ç­¾çš„æ–‡æœ¬
2. **æ£€æµ‹å·¥å…·è°ƒç”¨**ï¼švLLMæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨çš„ç»“æŸæ ‡ç­¾ï¼ˆå¦‚ `</tool>`ï¼‰
3. **è·¯ç”±åˆ°å·¥å…·**ï¼šæ ¹æ®å·¥å…·åç§°è·¯ç”±åˆ°å¯¹åº”çš„MCPå·¥å…·
4. **æ‰§è¡Œå·¥å…·**ï¼šé€šè¿‡MCPåè®®è°ƒç”¨åç«¯æœåŠ¡æ‰§è¡Œå·¥å…·
5. **è¿”å›ç»“æœ**ï¼šå·¥å…·ç»“æœè¢«åŒ…è£…å¹¶è¿”å›ç»™LLM
6. **ç»§ç»­ç”Ÿæˆ**ï¼šLLMåŸºäºå·¥å…·ç»“æœç»§ç»­ç”Ÿæˆå“åº”

### 7.9 å·¥å…·å®šä¹‰ä½ç½®æ€»ç»“

| ç»„ä»¶ | ä½ç½® |
|------|------|
| å·¥å…·æ³¨å†Œè¡¨ | `rl/open-instruct/open_instruct/search_utils/mcp_tools.py` |
| MCPåç«¯å·¥å…·å®ç° | `agent/dr_agent/mcp_backend/main.py` |
| å·¥å…·æ¥å£åŸºç±» | `agent/dr_agent/tool_interface/mcp_tools.py` |
| ç³»ç»Ÿæç¤ºè¯ | `rl/open-instruct/open_instruct/search_utils/system_prompts/unified_tool_calling_v20250907.yaml` |
| vLLMå·¥å…·é›†æˆ | `rl/open-instruct/open_instruct/tool_utils/tool_vllm.py` |
| å·¥å…·Actor | `rl/open-instruct/open_instruct/tool_utils/tool_actor.py` |

## å…«ã€å…³é”®å‚æ•°è¯´æ˜

- `rollout_batch_size`: æ¯ä¸ªè®­ç»ƒæ­¥éª¤é‡‡æ ·çš„promptæ•°é‡
- `number_samples_per_prompt`: æ¯ä¸ªpromptç”Ÿæˆçš„å“åº”æ•°é‡
- `num_epochs`: å¯¹åŒä¸€æ‰¹æ•°æ®è®­ç»ƒçš„è½®æ•°
- `num_mini_batches`: å°†rolloutæ•°æ®åˆ†æˆçš„å°æ‰¹é‡æ•°é‡
- `beta`: KLæ•£åº¦æƒ©ç½šç³»æ•°
- `cliprange`: PPOè£å‰ªèŒƒå›´
- `gamma`: GAEæŠ˜æ‰£å› å­
- `lam`: GAE lambdaå‚æ•°
- `max_tool_calls`: æ¯ä¸ªå·¥å…·çš„æœ€å¤§è°ƒç”¨æ¬¡æ•°
- `mcp_tool_names`: è¦å¯ç”¨çš„MCPå·¥å…·åç§°åˆ—è¡¨
- `mcp_parser_name`: å·¥å…·è°ƒç”¨è§£æå™¨ç‰ˆæœ¬

