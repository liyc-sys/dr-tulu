#!/usr/bin/env python3
"""
æµ‹è¯• OpenRouter ç›´æ¥è°ƒç”¨åŠŸèƒ½
ç¡®ä¿å¯ä»¥æ­£å¸¸ç”Ÿæˆ adaptive rubrics
"""

import os
import asyncio
import sys

os.environ["OPENAI_API_KEY"] = "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"


# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
if not os.environ.get("OPENAI_API_KEY"):
    print("è­¦å‘Š: OPENAI_API_KEY æœªè®¾ç½®")
    print("è¯·è®¾ç½®: export OPENAI_API_KEY='sk-or-v1-...'")
    sys.exit(1)

if not os.environ.get("OPENAI_API_BASE"):
    os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"

# å¼ºåˆ¶ä½¿ç”¨ OpenRouter ç›´æ¥è°ƒç”¨
os.environ["USE_OPENROUTER_DIRECT"] = "true"

# è®¾ç½®æµ‹è¯•æ¨¡å‹
if not os.environ.get("RUBRIC_JUDGE_MODEL"):
    os.environ["RUBRIC_JUDGE_MODEL"] = "openai/gpt-4o-mini"

from open_instruct.search_rewards.utils.run_utils import run_litellm_async, run_litellm


def print_section(title):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)


async def test_basic_call():
    """æµ‹è¯•åŸºæœ¬çš„ API è°ƒç”¨"""
    print_section("æµ‹è¯• 1: åŸºæœ¬ API è°ƒç”¨")
    
    try:
        response = await run_litellm_async(
            model_name=os.environ.get("RUBRIC_JUDGE_MODEL"),
            user_prompt="Say hello in one sentence.",
            max_tokens=100,
        )
        
        if response:
            print("âœ… åŸºæœ¬è°ƒç”¨æˆåŠŸ")
            print(f"å“åº”: {response[:100]}...")
            return True
        else:
            print("âŒ åŸºæœ¬è°ƒç”¨å¤±è´¥ï¼šè¿”å›ç©ºå­—ç¬¦ä¸²")
            return False
    except Exception as e:
        print(f"âŒ åŸºæœ¬è°ƒç”¨å¼‚å¸¸: {e}")
        return False


async def test_rubric_generation():
    """æµ‹è¯• rubric ç”Ÿæˆåœºæ™¯"""
    print_section("æµ‹è¯• 2: Rubric ç”Ÿæˆ")
    
    question = "What are the main causes of climate change?"
    responses = [
        "Climate change is primarily caused by human activities, especially the burning of fossil fuels.",
        "The sun is getting hotter, causing the Earth to warm up.",
        "Climate change is caused by greenhouse gas emissions from various sources.",
    ]
    
    prompt = f"""You are an expert evaluator. Generate evaluation rubrics for the following question and responses.

Question: {question}

Responses:
"""
    for i, resp in enumerate(responses):
        prompt += f"Response {i+1}: {resp}\n"
    
    prompt += """
Output in JSON format:
{
  "positive_rubrics": [{"title": "...", "description": "..."}],
  "negative_rubrics": [{"title": "...", "description": "..."}]
}
"""
    
    try:
        response = await run_litellm_async(
            model_name=os.environ.get("RUBRIC_JUDGE_MODEL"),
            user_prompt=prompt,
            max_tokens=2000,
        )
        
        if response:
            print("âœ… Rubric ç”ŸæˆæˆåŠŸ")
            print(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # å°è¯•è§£æ JSON
            import json
            try:
                # ç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å« JSON
                if "{" in response and "}" in response:
                    print("âœ… å“åº”åŒ…å« JSON æ ¼å¼æ•°æ®")
                    print(f"é¢„è§ˆ: {response[:200]}...")
                else:
                    print("âš ï¸  å“åº”ä¸åŒ…å« JSON æ ¼å¼")
            except:
                print("âš ï¸  æ— æ³•è§£æ JSON")
            
            return True
        else:
            print("âŒ Rubric ç”Ÿæˆå¤±è´¥ï¼šè¿”å›ç©ºå­—ç¬¦ä¸²")
            return False
    except Exception as e:
        print(f"âŒ Rubric ç”Ÿæˆå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_concurrent_calls():
    """æµ‹è¯•å¹¶å‘è°ƒç”¨"""
    print_section("æµ‹è¯• 3: å¹¶å‘è°ƒç”¨")
    
    num_concurrent = 5
    print(f"åŒæ—¶å‘èµ· {num_concurrent} ä¸ªè¯·æ±‚...")
    
    async def make_call(idx):
        response = await run_litellm_async(
            model_name=os.environ.get("RUBRIC_JUDGE_MODEL"),
            user_prompt=f"Count from 1 to {idx}.",
            max_tokens=50,
        )
        return idx, response
    
    try:
        tasks = [make_call(i) for i in range(1, num_concurrent + 1)]
        results = await asyncio.gather(*tasks)
        
        success_count = sum(1 for idx, resp in results if resp)
        print(f"âœ… æˆåŠŸ: {success_count}/{num_concurrent} ä¸ªè¯·æ±‚")
        
        if success_count == num_concurrent:
            print("âœ… æ‰€æœ‰å¹¶å‘è¯·æ±‚éƒ½æˆåŠŸ")
            return True
        else:
            print(f"âš ï¸  æœ‰ {num_concurrent - success_count} ä¸ªè¯·æ±‚å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ å¹¶å‘è°ƒç”¨å¼‚å¸¸: {e}")
        return False


def test_sync_call():
    """æµ‹è¯•åŒæ­¥è°ƒç”¨"""
    print_section("æµ‹è¯• 4: åŒæ­¥è°ƒç”¨")
    
    try:
        response = run_litellm(
            model_name=os.environ.get("RUBRIC_JUDGE_MODEL"),
            user_prompt="What is 2+2?",
            max_tokens=50,
        )
        
        if response:
            print("âœ… åŒæ­¥è°ƒç”¨æˆåŠŸ")
            print(f"å“åº”: {response[:100]}...")
            return True
        else:
            print("âŒ åŒæ­¥è°ƒç”¨å¤±è´¥ï¼šè¿”å›ç©ºå­—ç¬¦ä¸²")
            return False
    except Exception as e:
        print(f"âŒ åŒæ­¥è°ƒç”¨å¼‚å¸¸: {e}")
        return False


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print_section("æµ‹è¯• 5: é”™è¯¯å¤„ç†")
    
    # ä½¿ç”¨æ— æ•ˆçš„æ¨¡å‹åæµ‹è¯•é”™è¯¯å¤„ç†
    try:
        response = await run_litellm_async(
            model_name="invalid-model-name",
            user_prompt="This should fail",
            max_tokens=50,
            num_retries=2,  # å‡å°‘é‡è¯•æ¬¡æ•°åŠ å¿«æµ‹è¯•
        )
        
        # åº”è¯¥è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        if response == "":
            print("âœ… é”™è¯¯å¤„ç†æ­£ç¡®ï¼šè¿”å›ç©ºå­—ç¬¦ä¸²")
            return True
        else:
            print("âš ï¸  æ„å¤–æƒ…å†µï¼šæ— æ•ˆæ¨¡å‹è¿”å›äº†å“åº”")
            return True  # ä¹Ÿç®—é€šè¿‡ï¼Œå¯èƒ½æ˜¯å…œåº•æ¨¡å‹
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†å¤±è´¥ï¼šæŠ›å‡ºäº†å¼‚å¸¸ {e}")
        return False


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 70)
    print(" OpenRouter ç›´æ¥è°ƒç”¨æµ‹è¯•")
    print("=" * 70)
    print(f"API Base: {os.environ.get('OPENAI_API_BASE')}")
    print(f"Model: {os.environ.get('RUBRIC_JUDGE_MODEL')}")
    print(f"USE_OPENROUTER_DIRECT: {os.environ.get('USE_OPENROUTER_DIRECT')}")
    
    results = {}
    
    # è¿è¡Œæµ‹è¯•
    results["åŸºæœ¬è°ƒç”¨"] = await test_basic_call()
    results["Rubricç”Ÿæˆ"] = await test_rubric_generation()
    results["å¹¶å‘è°ƒç”¨"] = await test_concurrent_calls()
    results["åŒæ­¥è°ƒç”¨"] = test_sync_call()
    results["é”™è¯¯å¤„ç†"] = await test_error_handling()
    
    # æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")
    total = len(results)
    passed = sum(1 for v in results.values() if v)
    
    for test_name, passed_test in results.items():
        status = "âœ… é€šè¿‡" if passed_test else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")
    
    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")
        print("\nä½¿ç”¨æ–¹æ³•ï¼š")
        print("1. åœ¨ train_dr_tulu.sh ä¸­æ·»åŠ : export USE_OPENROUTER_DIRECT=true")
        print("2. ç¡®ä¿æ¨¡å‹ååŒ…å« provider å‰ç¼€: export RUBRIC_JUDGE_MODEL=openai/gpt-4o-mini")
        print("3. è¿è¡Œè®­ç»ƒè„šæœ¬")
        return 0
    else:
        print("\nâš ï¸  æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        print("\nå¸¸è§é—®é¢˜ï¼š")
        print("1. æ£€æŸ¥ OPENAI_API_KEY æ˜¯å¦æ­£ç¡®")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. æ£€æŸ¥æ¨¡å‹åæ˜¯å¦åŒ…å« provider å‰ç¼€")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

