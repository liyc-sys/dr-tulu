"""
PubMed è½¨è¿¹æ•°æ®ç”Ÿæˆå™¨ - ä¸»è„šæœ¬
ç”Ÿæˆï¼šé—®é¢˜ + GPT-5 å·¥å…·è°ƒç”¨è½¨è¿¹ + è¯„åˆ¤ rubrics

æµç¨‹ï¼š
1. ç”Ÿæˆé€‚åˆ pubmed_search çš„é—®é¢˜
2. è°ƒç”¨ GPT-5 è¿æ¥ MCP å·¥å…·ï¼Œç”Ÿæˆå·¥å…·è°ƒç”¨è½¨è¿¹
3. æ ¹æ®è½¨è¿¹ç»“æœç”Ÿæˆ content rubrics
"""
import asyncio
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict, field

SCRIPT_DIR = Path(__file__).parent.resolve()
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

from config import OUTPUT_DIR
from topic_generator import call_llm, extract_json
from trajectory_generator import (
    GPT5TrajectoryGenerator,
    Trajectory,
    generate_content_rubrics_from_trajectory
)


# Fixed tool usage rubrics (3 items, same for all data samples)
FIXED_TOOL_RUBRICS = [
    {
        "category": "tool_use",
        "title": "Correct pubmed_search usage",
        "description": "Model must call pubmed_search tool for literature search with correct parameter format",
        "weight": 3
    },
    {
        "category": "tool_use",
        "title": "Cite correct PMIDs",
        "description": "Output must contain correct PMIDs that align with tool return results",
        "weight": 3
    },
    {
        "category": "tool_use",
        "title": "Provide year and journal info",
        "description": "Each cited paper must include publication year and journal/venue name",
        "weight": 2
    },
]


# ä¸»é¢˜åˆ—è¡¨ï¼ˆæ¯æ¬¡éšæœºé€‰æ‹©ä¸€ä¸ªï¼‰
TOPIC_LIST = [
    "Cancer Treatment (targeted therapy, immunotherapy, chemotherapy resistance)",
    "Cardiovascular Disease (coronary artery disease, heart failure, atrial fibrillation)",
    "Neurological Disorders (Alzheimer's disease, Parkinson's disease, epilepsy)",
    "Infectious Diseases (COVID-19, HIV, antibiotic-resistant infections)",
    "Rare/Genetic Diseases (cystic fibrosis, ALS, Huntington's disease)",
    "Drug Development/Clinical Trials (Phase III trials, drug interactions)",
    "Metabolic Diseases (diabetes, obesity, NAFLD)",
    "Autoimmune Diseases (rheumatoid arthritis, SLE, multiple sclerosis)",
    "Cancer Immunotherapy (CAR-T, PD-1/PD-L1 inhibitors, tumor microenvironment)",
    "Gene & Cell Therapy (CRISPR, stem cells, gene editing)",
]

# é—®é¢˜ç”Ÿæˆ Prompt
QUESTION_GENERATION_PROMPT = """You are a medical research question generation expert. Please generate {num_questions} research questions suitable for answering using PubMed medical literature search.

**Requirements**:
1. Questions must require consulting PubMed papers to answer accurately
2. Questions involve specific medical/biomedical research (diseases, drugs, mechanisms, treatments, etc.)
3. **IMPORTANT: Do NOT include specific PMIDs, paper IDs, or study names in questions**
4. Questions should be general but specific enough to require literature search
5. Cover different types: efficacy comparison, mechanism research, epidemiology, prognosis analysis, reviews
6. Language: **English only**
7. Moderate difficulty, requiring 2-5 papers to answer comprehensively
8. Each must be a single clear question, not multiple compound questions

**AVOID these patterns** (do NOT include in questions):
- âŒ "According to PMID 12345678..."
- âŒ "Based on the Smith et al. 2023 study..."
- âŒ "In the KEYNOTE-001 trial..."
- âŒ "Refer to paper PMID:98765432..."

Question examples (GOOD):
First example: In first-line treatment of advanced melanoma, compare the efficacy and toxicity differences between PD-1 inhibitor monotherapy versus PD-1 + CTLA-4 combination therapy.

Second example: In immune checkpoint therapy, how do tumor mutational burden (TMB) and tumor microenvironment (such as CD8+ T cell infiltration) correlate with treatment efficacy? Are they independent or do they interact?

**Assigned topic for this batch**: {topic}
Generate all questions around this topic, ensuring questions are specific, in-depth, and non-repetitive.


**Output JSON format**:
```json
{{
  "questions": [
    {{
      "question": "Question content in English",
      "topic": "Topic classification",
      "question_type": "efficacy_comparison/mechanism/epidemiology/prognosis/review",
      "expected_search_terms": ["possible PubMed search terms"]
    }}
  ]
}}
```

Output JSON only, no other content.
"""


@dataclass
class TrajectoryDataSample:
    """å®Œæ•´çš„è½¨è¿¹æ•°æ®æ ·æœ¬"""
    sample_id: str
    question: str
    topic: str
    question_type: str
    trajectory: Dict[str, Any]  # GPT-5 ç”Ÿæˆçš„è½¨è¿¹
    tool_rubrics: Optional[List[Dict]] = None  # å›ºå®šçš„å·¥å…·è°ƒç”¨ rubricsï¼ˆå¯é€‰ï¼‰
    content_rubrics: Optional[List[Dict]] = None  # åŠ¨æ€ç”Ÿæˆçš„å†…å®¹ rubricsï¼ˆå¯é€‰ï¼‰
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        result = {
            "sample_id": self.sample_id,
            "question": self.question,
            "topic": self.topic,
            "question_type": self.question_type,
            "trajectory": self.trajectory,
            "metadata": self.metadata
        }
        
        # åªæœ‰åœ¨æœ‰ rubrics æ—¶æ‰æ·»åŠ 
        if self.tool_rubrics is not None and self.content_rubrics is not None:
            result["tool_rubrics"] = self.tool_rubrics
            result["content_rubrics"] = self.content_rubrics
            result["all_rubrics"] = self.tool_rubrics + self.content_rubrics
        
        return result


class TrajectoryDatasetGenerator:
    """è½¨è¿¹æ•°æ®é›†ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        model: str = "openai/gpt-5.2",
        # mini_model: str = "openai/gpt-5-mini",  # ç”¨äºæ¬¡è¦ä»»åŠ¡çš„ä¾¿å®œæ¨¡å‹
        mini_model: str = "openai/gpt-5.2",
        num_questions: int = 10,
        language: str = "en",  # é»˜è®¤è‹±æ–‡
        output_dir: str = OUTPUT_DIR,
        incremental_save: bool = True,
        generate_rubrics: bool = True  # æ˜¯å¦ç”Ÿæˆ rubrics
    ):
        self.model = model  # ç”¨äºè½¨è¿¹ç”Ÿæˆï¼ˆé‡è¦ï¼‰
        self.mini_model = mini_model  # ç”¨äºé—®é¢˜ç”Ÿæˆå’Œ rubrics ç”Ÿæˆï¼ˆæ¬¡è¦ï¼‰
        self.num_questions = num_questions
        self.language = language
        self.generate_rubrics = generate_rubrics  # æ§åˆ¶æ˜¯å¦ç”Ÿæˆ rubrics
        self.trajectory_generator = GPT5TrajectoryGenerator(model=model)
        self.samples: List[TrajectoryDataSample] = []
        self.output_dir = output_dir
        self.incremental_save = incremental_save
        
        # åˆ›å»ºè¾“å‡ºç›®å½•å’Œå¢é‡ä¿å­˜æ–‡ä»¶
        if self.incremental_save:
            os.makedirs(self.output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # æ ¹æ®æ˜¯å¦ç”Ÿæˆ rubrics ä½¿ç”¨ä¸åŒçš„æ–‡ä»¶å
            suffix = "incremental" if self.generate_rubrics else "no_rubrics_incremental"
            self.incremental_file = os.path.join(
                self.output_dir, 
                f"pubmed_trajectory_{timestamp}_{suffix}.jsonl"
            )
            self.questions_incremental_file = os.path.join(
                self.output_dir,
                f"questions_{timestamp}_{suffix}.jsonl"
            )
            self.timestamp = timestamp
    
    async def generate_questions(self) -> List[Dict]:
        """Step 1: ç”Ÿæˆé€‚åˆ pubmed æœç´¢çš„é—®é¢˜ï¼ˆä¸»é¢˜å‡åŒ€åˆ†å¸ƒï¼‰"""
        import random
        
        print("\n" + "=" * 60)
        print("Step 1: ç”Ÿæˆé—®é¢˜")
        print("=" * 60)
        
        if self.incremental_save:
            print(f"ğŸ’¾ é—®é¢˜å¢é‡ä¿å­˜å·²å¯ç”¨: {self.questions_incremental_file}")
        
        all_questions = []
        num_topics = len(TOPIC_LIST)
        
        # è®¡ç®—æ¯ä¸ªä¸»é¢˜åº”è¯¥ç”Ÿæˆçš„é—®é¢˜æ•°é‡
        base_count = self.num_questions // num_topics  # æ¯ä¸ªä¸»é¢˜çš„åŸºç¡€æ•°é‡
        remainder = self.num_questions % num_topics     # ä½™æ•°åˆ†é…ç»™å‰å‡ ä¸ªä¸»é¢˜
        
        print(f"æ€»å…±éœ€è¦ {self.num_questions} ä¸ªé—®é¢˜ï¼Œåˆ†å¸ƒåˆ° {num_topics} ä¸ªä¸»é¢˜")
        
        # æ‰“ä¹±ä¸»é¢˜é¡ºåºï¼Œé¿å…æ¯æ¬¡éƒ½ä»åŒä¸€ä¸ªä¸»é¢˜å¼€å§‹
        shuffled_topics = TOPIC_LIST.copy()
        random.shuffle(shuffled_topics)
        
        for i, topic in enumerate(shuffled_topics):
            # å‰ remainder ä¸ªä¸»é¢˜å¤šç”Ÿæˆ 1 ä¸ªé—®é¢˜
            questions_for_topic = base_count + (1 if i < remainder else 0)
            
            if questions_for_topic == 0:
                continue
            
            print(f"\nä¸»é¢˜ [{i+1}/{num_topics}]: {topic}")
            print(f"  è®¡åˆ’ç”Ÿæˆ {questions_for_topic} ä¸ªé—®é¢˜...")
            
            prompt = QUESTION_GENERATION_PROMPT.format(
                num_questions=questions_for_topic,
                topic=topic
            )
            
            try:
                # ä½¿ç”¨ model ç”Ÿæˆé—®é¢˜
                response = await call_llm(prompt, temperature=0.7, model=self.model)
                result = extract_json(response)
                questions = result.get("questions", [])
                all_questions.extend(questions)
                
                # å¢é‡ä¿å­˜é—®é¢˜
                self.append_questions_to_file(questions)
                
                print(f"  âœ“ ç”Ÿæˆäº† {len(questions)} ä¸ªé—®é¢˜")
            except Exception as e:
                print(f"  âœ— ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            # é¿å… API é™æµ
            await asyncio.sleep(1)
        
        print(f"\nâœ“ æ€»å…±ç”Ÿæˆäº† {len(all_questions)} ä¸ªé—®é¢˜")
        return all_questions
    
    async def generate_trajectory_for_question(
        self,
        question_data: Dict,
        sample_index: int
    ) -> Optional[TrajectoryDataSample]:
        """Step 2 & 3: ä¸ºå•ä¸ªé—®é¢˜ç”Ÿæˆè½¨è¿¹å’Œ rubricsï¼ˆå¯é€‰ï¼‰"""
        question = question_data["question"]
        
        print(f"\n[{sample_index}] é—®é¢˜: {question}")
        
        # Step 2: ç”Ÿæˆè½¨è¿¹
        print("  æ­£åœ¨ç”Ÿæˆå·¥å…·è°ƒç”¨è½¨è¿¹...")
        try:
            trajectory = await self.trajectory_generator.generate_trajectory(question)
            print(f"  âœ“ è½¨è¿¹ç”Ÿæˆå®Œæˆ: {trajectory.total_tool_calls} æ¬¡å·¥å…·è°ƒç”¨")
        except Exception as e:
            print(f"  âœ— è½¨è¿¹ç”Ÿæˆå¤±è´¥: {e}")
            return None
        
        # Step 3: æ ¹æ®è½¨è¿¹ç”Ÿæˆ content rubricsï¼ˆå¦‚æœå¯ç”¨ï¼‰
        tool_rubrics = None
        content_rubrics = None
        
        if self.generate_rubrics:
            print("  æ­£åœ¨ç”Ÿæˆå†…å®¹ rubrics...")
            try:
                content_rubrics = await generate_content_rubrics_from_trajectory(
                    question, trajectory, model=self.mini_model
                )
                print(f"  âœ“ ç”Ÿæˆäº† {len(content_rubrics)} æ¡å†…å®¹ rubrics")
            except Exception as e:
                print(f"  âš  å†…å®¹ rubrics ç”Ÿæˆå¤±è´¥: {e}")
                content_rubrics = []
            
            tool_rubrics = FIXED_TOOL_RUBRICS.copy()
        else:
            print("  âŠ˜ è·³è¿‡ rubrics ç”Ÿæˆ")
        
        return TrajectoryDataSample(
            sample_id=f"pubmed_traj_{sample_index:05d}",
            question=question,
            topic=question_data.get("topic", ""),
            question_type=question_data.get("question_type", ""),
            trajectory=trajectory.to_dict(),
            tool_rubrics=tool_rubrics,
            content_rubrics=content_rubrics,
            metadata={
                "expected_search_terms": question_data.get("expected_search_terms", []),
                "tools_used": trajectory.tools_used,
                "total_tool_calls": trajectory.total_tool_calls,
                "generation_time": datetime.now().isoformat()
            }
        )
    
    async def generate_trajectory_with_retry(
        self,
        q_data: Dict,
        sample_index: int,
        semaphore: asyncio.Semaphore,
        max_retries: int = 3
    ) -> Optional[TrajectoryDataSample]:
        """å¸¦é‡è¯•æœºåˆ¶çš„è½¨è¿¹ç”Ÿæˆï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            for attempt in range(max_retries):
                try:
                    sample = await self.generate_trajectory_for_question(q_data, sample_index)
                    if sample:
                        return sample
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
                        print(f"  âš ï¸ [{sample_index}] å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        print(f"  â³ ç­‰å¾… {wait_time}s åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"  âœ— [{sample_index}] æ‰€æœ‰é‡è¯•å¤±è´¥: {e}")
            return None
    
    async def generate_dataset(self, concurrency: int = 5) -> List[TrajectoryDataSample]:
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
        
        Args:
            concurrency: å¹¶å‘æ•°ï¼Œå»ºè®® 3-10ï¼ˆå–å†³äº MCP æœåŠ¡å™¨å’Œ API é™åˆ¶ï¼‰
        """
        print("\n" + "=" * 60)
        print("PubMed è½¨è¿¹æ•°æ®ç”Ÿæˆå™¨")
        print("=" * 60)
        print(f"æ¨¡å‹: {self.model}")
        print(f"è®¡åˆ’ç”Ÿæˆ: {self.num_questions} ä¸ªé—®é¢˜")
        print(f"å¹¶å‘æ•°: {concurrency}")
        
        # Step 1: ç”Ÿæˆé—®é¢˜
        questions = await self.generate_questions()
        
        if not questions:
            print("âœ— æ²¡æœ‰ç”Ÿæˆä»»ä½•é—®é¢˜")
            return []
        
        # Step 2 & 3: å¹¶å‘ç”Ÿæˆè½¨è¿¹å’Œ rubrics
        print("\n" + "=" * 60)
        print("Step 2 & 3: ç”Ÿæˆè½¨è¿¹å’Œ rubricsï¼ˆå¹¶å‘æ¨¡å¼ï¼‰")
        print("=" * 60)
        
        # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
        semaphore = asyncio.Semaphore(concurrency)
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = []
        for i, q_data in enumerate(questions, 1):
            task = self.generate_trajectory_with_retry(q_data, i, semaphore)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œï¼Œæ˜¾ç¤ºè¿›åº¦
        samples = []
        completed = 0
        total = len(tasks)
        
        if self.incremental_save:
            print(f"ğŸ’¾ å¢é‡ä¿å­˜å·²å¯ç”¨: {self.incremental_file}")
        
        for coro in asyncio.as_completed(tasks):
            sample = await coro
            completed += 1
            if sample:
                samples.append(sample)
                # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¢é‡ä¿å­˜ï¼‰
                self.append_sample_to_file(sample)
            
            # æ˜¾ç¤ºè¿›åº¦
            success_rate = (len(samples) / completed * 100) if completed > 0 else 0
            print(f"\nğŸ“Š è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                  f"æˆåŠŸ: {len(samples)} | å¤±è´¥: {completed - len(samples)} | "
                  f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        self.samples = samples
        print(f"\nâœ“ å®Œæˆï¼å…±ç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬")
        if self.incremental_save:
            print(f"ğŸ’¾ æ‰€æœ‰æ ·æœ¬å·²å¢é‡ä¿å­˜åˆ°: {self.incremental_file}")
        return samples
    
    def append_questions_to_file(self, questions: List[Dict]):
        """å¢é‡ä¿å­˜ï¼šè¿½åŠ é—®é¢˜åˆ°æ–‡ä»¶"""
        if not self.incremental_save or not questions:
            return
        
        try:
            with open(self.questions_incremental_file, 'a', encoding='utf-8') as f:
                for q in questions:
                    f.write(json.dumps(q, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"  âš ï¸ é—®é¢˜å¢é‡ä¿å­˜å¤±è´¥: {e}")
    
    def append_sample_to_file(self, sample: TrajectoryDataSample):
        """å¢é‡ä¿å­˜ï¼šè¿½åŠ å•æ¡æ ·æœ¬åˆ°æ–‡ä»¶"""
        if not self.incremental_save:
            return
        
        try:
            with open(self.incremental_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(sample.to_dict(), ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"  âš ï¸ å¢é‡ä¿å­˜å¤±è´¥: {e}")
    
    def save_checkpoint(self, output_dir: str = OUTPUT_DIR, checkpoint_name: str = "checkpoint"):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if not self.samples:
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        checkpoint_path = os.path.join(output_dir, f"{checkpoint_name}.jsonl")
        
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            for sample in self.samples:
                f.write(json.dumps(sample.to_dict(), ensure_ascii=False) + '\n')
        
        return checkpoint_path
    
    def save_dataset(self, output_dir: str = None):
        """ä¿å­˜æ•°æ®é›†"""
        if output_dir is None:
            output_dir = self.output_dir
        
        os.makedirs(output_dir, exist_ok=True)
        
        # å¦‚æœä½¿ç”¨äº†å¢é‡ä¿å­˜ï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰æ–‡ä»¶
        if self.incremental_save and hasattr(self, 'incremental_file'):
            jsonl_path = self.incremental_file
            print(f"âœ“ JSONL (å¢é‡ä¿å­˜): {jsonl_path}")
            timestamp = self.timestamp
        else:
            # å¦åˆ™ä¸€æ¬¡æ€§ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            jsonl_path = os.path.join(output_dir, f"pubmed_trajectory_{timestamp}.jsonl")
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for sample in self.samples:
                    f.write(json.dumps(sample.to_dict(), ensure_ascii=False) + '\n')
            print(f"âœ“ ä¿å­˜ JSONL: {jsonl_path}")
        
        # ä¿å­˜ CSVï¼ˆå…¼å®¹ç°æœ‰è®­ç»ƒæ ¼å¼ï¼‰
        csv_path = os.path.join(output_dir, f"pubmed_trajectory_{timestamp}.csv")
        self._save_as_csv(csv_path)
        print(f"âœ“ ä¿å­˜ CSV: {csv_path}")
        
        # ä¿å­˜ç»Ÿè®¡
        stats = self._generate_stats()
        stats_path = os.path.join(output_dir, f"trajectory_stats_{timestamp}.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ä¿å­˜ç»Ÿè®¡: {stats_path}")
        
        return jsonl_path
    
    def _save_as_csv(self, csv_path: str):
        """ä¿å­˜ä¸º CSV æ ¼å¼"""
        import csv
        
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'source', 'question_type', 'messages', 'ground_truth', 'dataset'
            ])
            writer.writeheader()
            
            for sample in self.samples:
                # æ„é€  messagesï¼šuser é—®é¢˜ + assistant çš„å®Œæ•´ interleaved è½¨è¿¹
                messages = [
                    {"content": sample.question, "role": "user"},
                    {"content": sample.trajectory.get("interleaved_text", ""), "role": "assistant"}
                ]
                
                ground_truth = {
                    "query": sample.question,
                    "interleaved_trajectory": sample.trajectory.get("interleaved_text", ""),
                    "tool_calls": sample.trajectory.get("tool_calls", []),
                    "final_answer": sample.trajectory.get("final_answer", ""),
                    "pmids_cited": sample.trajectory.get("pmids_cited", []),
                    "total_tool_calls": sample.trajectory.get("total_tool_calls", 0),
                    "tools_used": sample.trajectory.get("tools_used", [])
                }
                
                # åªæœ‰åœ¨æœ‰ rubrics æ—¶æ‰æ·»åŠ 
                if sample.tool_rubrics is not None and sample.content_rubrics is not None:
                    ground_truth["rubrics"] = sample.tool_rubrics + sample.content_rubrics
                    ground_truth["tool_rubrics"] = sample.tool_rubrics
                    ground_truth["content_rubrics"] = sample.content_rubrics
                
                writer.writerow({
                    'source': 'pubmed_trajectory_generator',
                    'question_type': 'pubmed_search',
                    'messages': json.dumps(messages, ensure_ascii=False),
                    'ground_truth': json.dumps(ground_truth, ensure_ascii=False),
                    'dataset': 'pubmed_trajectory'
                })
    
    def _generate_stats(self) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.samples:
            return {}
        
        topics = {}
        question_types = {}
        tool_calls_counts = []
        content_rubrics_counts = []
        
        for sample in self.samples:
            topics[sample.topic] = topics.get(sample.topic, 0) + 1
            question_types[sample.question_type] = question_types.get(sample.question_type, 0) + 1
            tool_calls_counts.append(sample.metadata.get("total_tool_calls", 0))
            # åªæœ‰åœ¨æœ‰ rubrics æ—¶æ‰ç»Ÿè®¡
            if sample.content_rubrics is not None:
                content_rubrics_counts.append(len(sample.content_rubrics))
        
        stats = {
            "total_samples": len(self.samples),
            "topics": topics,
            "question_types": question_types,
            "avg_tool_calls": sum(tool_calls_counts) / len(tool_calls_counts) if tool_calls_counts else 0,
            "generation_time": datetime.now().isoformat(),
            "has_rubrics": self.generate_rubrics
        }
        
        # åªæœ‰åœ¨ç”Ÿæˆäº† rubrics æ—¶æ‰æ·»åŠ  rubrics ç»Ÿè®¡
        if content_rubrics_counts:
            stats["avg_content_rubrics"] = sum(content_rubrics_counts) / len(content_rubrics_counts)
        
        return stats


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ PubMed è½¨è¿¹æ•°æ®é›†ï¼ˆæ”¯æŒå¹¶å‘ + å¢é‡ä¿å­˜ï¼‰")
    parser.add_argument("--model", type=str, default="openai/gpt-4o", help="è½¨è¿¹ç”Ÿæˆç”¨çš„ä¸»æ¨¡å‹ï¼ˆé‡è¦ï¼‰")
    parser.add_argument("--mini-model", type=str, default="openai/gpt-5-mini", help="é—®é¢˜å’Œrubricsç”Ÿæˆç”¨çš„æ¬¡è¦æ¨¡å‹ï¼ˆèŠ‚çœæˆæœ¬ï¼‰")
    parser.add_argument("--num-questions", type=int, default=5, help="é—®é¢˜æ•°é‡")
    parser.add_argument("--language", type=str, default="zh", choices=["zh", "en"], help="è¯­è¨€")
    parser.add_argument("--output", type=str, default=OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--concurrency", type=int, default=5, help="å¹¶å‘æ•°ï¼ˆå»ºè®® 3-10ï¼Œå–å†³äº MCP æœåŠ¡å™¨è´Ÿè½½ï¼‰")
    parser.add_argument("--no-incremental", action="store_true", help="ç¦ç”¨å¢é‡ä¿å­˜ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    parser.add_argument("--no-rubrics", action="store_true", help="ç¦ç”¨ rubrics ç”Ÿæˆï¼ˆé»˜è®¤ç”Ÿæˆï¼‰")
    
    args = parser.parse_args()
    
    print(f"ä¸»æ¨¡å‹ï¼ˆè½¨è¿¹ç”Ÿæˆï¼‰: {args.model}")
    print(f"æ¬¡è¦æ¨¡å‹ï¼ˆé—®é¢˜/rubricsï¼‰: {args.mini_model}")
    print(f"å¹¶å‘æ•°: {args.concurrency}")
    print(f"å¢é‡ä¿å­˜: {'ç¦ç”¨' if args.no_incremental else 'å¯ç”¨'}")
    print(f"Rubrics ç”Ÿæˆ: {'ç¦ç”¨' if args.no_rubrics else 'å¯ç”¨'}")
    
    generator = TrajectoryDatasetGenerator(
        model=args.model,
        mini_model=args.mini_model,
        num_questions=args.num_questions,
        language=args.language,
        output_dir=args.output,
        incremental_save=not args.no_incremental,
        generate_rubrics=not args.no_rubrics
    )
    
    try:
        await generator.generate_dataset(concurrency=args.concurrency)
        generator.save_dataset(args.output)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²å®Œæˆçš„æ ·æœ¬...")
        if generator.samples:
            generator.save_dataset(args.output)
        print("âœ“ å·²ä¿å­˜éƒ¨åˆ†ç»“æœ")
    except Exception as e:
        print(f"\n\nâœ— ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        if generator.samples:
            print("\nä¿å­˜å·²å®Œæˆçš„æ ·æœ¬...")
            generator.save_dataset(args.output)
    
    print("\n" + "=" * 60)
    print("æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

