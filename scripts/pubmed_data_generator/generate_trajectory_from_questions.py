"""
ä½¿ç”¨æœ¬åœ°Qwen3-8Bæ¨¡å‹ä»å·²æœ‰é—®é¢˜ç”Ÿæˆè½¨è¿¹
ä»JSONLæ–‡ä»¶è¯»å–questionsï¼Œè°ƒç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆè½¨è¿¹ï¼Œä¸ç”Ÿæˆrubrics
"""
import asyncio
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

SCRIPT_DIR = Path(__file__).parent.resolve()
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

from config import OUTPUT_DIR, MCP_HOST, MCP_PORT
from trajectory_generator import (
    MCPToolExecutor,
    SYSTEM_PROMPT,
    ToolCallRecord,
    Trajectory
)
import httpx
import re


class LocalModelTrajectoryGenerator:
    """ä½¿ç”¨æœ¬åœ°Qwen3-8Bæ¨¡å‹ç”Ÿæˆè½¨è¿¹"""
    
    def __init__(
        self,
        local_model_url: str = "http://localhost:8000/v1",  # æœ¬åœ°æ¨¡å‹APIåœ°å€
        model_name: str = "Qwen3-8B",
        max_turns: int = 10
    ):
        self.local_model_url = local_model_url
        self.model_name = model_name
        self.max_turns = max_turns
        self.tool_executor = MCPToolExecutor(host=MCP_HOST, port=MCP_PORT)
    
    def _remove_hallucinated_tool_output(self, content: str) -> str:
        """ç§»é™¤æ¨¡å‹å¯èƒ½ç”Ÿæˆçš„å‡ tool_output å†…å®¹"""
        pattern = r'(</call_tool>)\s*<tool_output>.*?(?:</tool_output>|$)'
        cleaned = re.sub(pattern, r'\1', content, flags=re.DOTALL)
        
        if '<tool_output>' in cleaned:
            idx = cleaned.find('<tool_output>')
            cleaned = cleaned[:idx].rstrip()
        
        return cleaned
    
    def _clean_model_output(self, content: str, first_tool_call: tuple) -> str:
        """æ¸…ç†æ¨¡å‹è¾“å‡ºï¼Œåªä¿ç•™åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ call_tool ä¸ºæ­¢"""
        tool_name, params_str, query = first_tool_call
        
        if params_str:
            call_tool_tag = f'<call_tool name="{tool_name}" {params_str}>{query}</call_tool>'
        else:
            call_tool_tag = f'<call_tool name="{tool_name}">{query}</call_tool>'
        
        first_call_idx = content.find('<call_tool')
        if first_call_idx == -1:
            return content
        
        prefix = content[:first_call_idx]
        
        close_tag_idx = content.find('</call_tool>', first_call_idx)
        if close_tag_idx != -1:
            clean_content = content[:close_tag_idx + len('</call_tool>')]
        else:
            clean_content = prefix + call_tool_tag
        
        clean_content = self._remove_hallucinated_tool_output(clean_content)
        
        return clean_content
    
    async def generate_trajectory(self, question: str) -> Trajectory:
        """ä¸ºç»™å®šé—®é¢˜ç”Ÿæˆå®Œæ•´çš„ interleaved è½¨è¿¹"""
        
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": question}
        ]
        
        tool_calls = []
        tools_used = set()
        total_tool_calls = 0
        interleaved_parts = []
        final_answer = ""
        
        for turn in range(self.max_turns):
            # è°ƒç”¨æœ¬åœ° LLM
            response = await self._call_local_llm(messages)
            
            if not response:
                print(f"  âš  LLM æ— å“åº”ï¼Œåœæ­¢ç”Ÿæˆ")
                break
            
            content = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            if not content:
                print(f"  âš  å“åº”å†…å®¹ä¸ºç©ºï¼Œåœæ­¢ç”Ÿæˆ")
                break
            
            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            tool_call_matches = re.findall(
                r'<call_tool\s+name="([^"]+)"(?:\s+([^>]*))?>([^<]*)</call_tool>',
                content
            )
            
            # å¦‚æœæ²¡æœ‰é—­åˆæ ‡ç­¾çš„åŒ¹é…ï¼Œå°è¯•åŒ¹é…æœªé—­åˆçš„
            if not tool_call_matches:
                unclosed_matches = re.findall(
                    r'<call_tool\s+name="([^"]+)"(?:\s+([^>]*))?>(.*?)(?=<call_tool|<answer|$)',
                    content, re.DOTALL
                )
                if unclosed_matches:
                    first_match = unclosed_matches[0]
                    tool_name = first_match[0]
                    params_str = first_match[1]
                    query = first_match[2].strip().split('\n')[0].strip()
                    tool_call_matches = [(tool_name, params_str, query)]
                    print(f"  âš  æ£€æµ‹åˆ°æœªé—­åˆçš„ <call_tool>ï¼Œè‡ªåŠ¨ä¿®å¤")
            
            if tool_call_matches:
                # æ¸…ç†å†…å®¹
                clean_content = self._clean_model_output(content, tool_call_matches[0])
                interleaved_parts.append(clean_content)
                
                # åªæ‰§è¡Œç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
                all_tool_outputs = []
                for tool_name, params_str, query in tool_call_matches[:1]:
                    # è§£æå‚æ•°
                    parameters = {}
                    if params_str:
                        param_matches = re.findall(r'(\w+)="([^"]*)"', params_str)
                        for k, v in param_matches:
                            try:
                                parameters[k] = int(v)
                            except:
                                parameters[k] = v
                    
                    query = query.strip()
                    tools_used.add(tool_name)
                    total_tool_calls += 1
                    
                    print(f"  æ‰§è¡Œå·¥å…·: {tool_name}({query})")
                    
                    # æ‰§è¡Œå·¥å…·
                    raw_result, formatted_output = await self.tool_executor.execute_tool(
                        tool_name, parameters, query
                    )
                    
                    tool_calls.append(ToolCallRecord(
                        tool_name=tool_name,
                        parameters=parameters,
                        query=query,
                        result=self._truncate_result(raw_result),
                        timestamp=datetime.now().isoformat()
                    ))
                    
                    all_tool_outputs.append(formatted_output)
                
                # æ·»åŠ å·¥å…·è¾“å‡º
                tool_output_text = "\n".join(all_tool_outputs)
                interleaved_parts.append(tool_output_text)
                
                messages.append({"role": "assistant", "content": clean_content})
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å·¥å…·è°ƒç”¨é™åˆ¶
                if total_tool_calls >= 5:
                    reminder = f"{tool_output_text}\n\nâš ï¸ You have reached the maximum limit of 5 tool calls. You MUST provide your final answer now using the <answer> tag."
                    messages.append({"role": "user", "content": reminder})
                    print(f"  âš ï¸ å·²è¾¾åˆ°å·¥å…·è°ƒç”¨ä¸Šé™ (5æ¬¡)ï¼Œæé†’æ¨¡å‹ç»™å‡ºç­”æ¡ˆ")
                else:
                    messages.append({"role": "user", "content": tool_output_text})
                
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ <answer>
                if "<answer>" in content:
                    interleaved_parts.append(content)
                    answer_match = re.search(r'<answer>(.*?)</answer>', content, re.DOTALL)
                    if answer_match:
                        final_answer = answer_match.group(1).strip()
                    else:
                        final_answer = content.split("<answer>")[-1].strip()
                    print(f"  âœ“ è·å–åˆ°æœ€ç»ˆç­”æ¡ˆ")
                    break
                else:
                    interleaved_parts.append(content)
                    messages.append({"role": "assistant", "content": content})
                    messages.append({"role": "user", "content": "Please continue with tool calls or provide your final answer."})
        
        # ç»„åˆå®Œæ•´çš„ interleaved æ–‡æœ¬
        interleaved_text = "\n".join(interleaved_parts)
        
        # æå–æ‰€æœ‰å¼•ç”¨çš„ PMIDs
        pmids_cited = list(set(re.findall(r'<cite\s+id="(\d+)"', interleaved_text)))
        
        return Trajectory(
            question=question,
            interleaved_text=interleaved_text,
            tool_calls=tool_calls,
            final_answer=final_answer,
            total_tool_calls=total_tool_calls,
            tools_used=list(tools_used),
            pmids_cited=pmids_cited
        )
    
    async def _call_local_llm(self, messages: List[Dict]) -> Optional[Dict]:
        """è°ƒç”¨æœ¬åœ°æ¨¡å‹ APIï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰"""
        request_data = {
            "model": self.model_name,
            "messages": messages,
            "temperature": 0.1,
            "max_tokens": 1024,
            "stop": ["</call_tool>\n", "</call_tool><", "<tool_output>"],
        }
        
        async with httpx.AsyncClient(timeout=180.0) as client:
            try:
                response = await client.post(
                    f"{self.local_model_url}/chat/completions",
                    headers={
                        "Content-Type": "application/json; charset=utf-8",
                    },
                    content=json.dumps(request_data, ensure_ascii=False).encode('utf-8'),
                )
                response.raise_for_status()
                return response.json()
            except Exception as e:
                print(f"æœ¬åœ°LLMè°ƒç”¨å¤±è´¥: {e}")
                return None
    
    def _truncate_result(self, result: Any, max_length: int = 3000) -> Any:
        """æˆªæ–­è¿‡é•¿çš„ç»“æœ"""
        if isinstance(result, dict):
            if "data" in result:
                truncated_data = []
                for paper in result.get("data", [])[:5]:
                    if isinstance(paper, dict):
                        truncated_data.append({
                            "paperId": paper.get("paperId"),
                            "title": paper.get("title"),
                            "abstract": paper.get("abstract", ""),
                            "year": paper.get("year"),
                            "venue": paper.get("venue"),
                        })
                return {"total": result.get("total"), "data": truncated_data}
            
            result_str = json.dumps(result, ensure_ascii=False)
            if len(result_str) > max_length:
                return {"truncated": True, "preview": result_str[:max_length]}
        return result


@dataclass
class TrajectoryDataSample:
    """è½¨è¿¹æ•°æ®æ ·æœ¬ï¼ˆæ— rubricsï¼‰"""
    sample_id: str
    question: str
    topic: str
    question_type: str
    trajectory: Dict[str, Any]
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict:
        return {
            "sample_id": self.sample_id,
            "question": self.question,
            "topic": self.topic,
            "question_type": self.question_type,
            "trajectory": self.trajectory,
            "metadata": self.metadata
        }


class QuestionBasedTrajectoryGenerator:
    """ä»å·²æœ‰é—®é¢˜ç”Ÿæˆè½¨è¿¹"""
    
    def __init__(
        self,
        questions_file: str,
        local_model_url: str = "http://localhost:8000/v1",
        model_name: str = "Qwen3-8B",
        output_dir: str = OUTPUT_DIR,
        incremental_save: bool = True
    ):
        self.questions_file = questions_file
        self.trajectory_generator = LocalModelTrajectoryGenerator(
            local_model_url=local_model_url,
            model_name=model_name
        )
        self.samples: List[TrajectoryDataSample] = []
        self.output_dir = output_dir
        self.incremental_save = incremental_save
        
        # åˆ›å»ºè¾“å‡ºç›®å½•å’Œå¢é‡ä¿å­˜æ–‡ä»¶
        if self.incremental_save:
            os.makedirs(self.output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_suffix = model_name.replace("/", "_").replace(":", "_")
            self.incremental_file = os.path.join(
                self.output_dir,
                f"pubmed_trajectory_{timestamp}_{model_suffix}_incremental.jsonl"
            )
            self.timestamp = timestamp
            self.model_suffix = model_suffix
    
    def load_questions(self) -> List[Dict]:
        """ä»JSONLæ–‡ä»¶åŠ è½½é—®é¢˜"""
        print(f"ä»æ–‡ä»¶åŠ è½½é—®é¢˜: {self.questions_file}")
        questions = []
        
        with open(self.questions_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        q = json.loads(line)
                        questions.append(q)
                    except json.JSONDecodeError as e:
                        print(f"  âš  JSONè§£æé”™è¯¯: {e}")
        
        print(f"âœ“ åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
        return questions
    
    async def generate_trajectory_for_question(
        self,
        question_data: Dict,
        sample_index: int
    ) -> Optional[TrajectoryDataSample]:
        """ä¸ºå•ä¸ªé—®é¢˜ç”Ÿæˆè½¨è¿¹"""
        question = question_data.get("question", "")
        
        print(f"\n[{sample_index}] é—®é¢˜: {question}")
        
        # ç”Ÿæˆè½¨è¿¹
        print("  æ­£åœ¨ç”Ÿæˆå·¥å…·è°ƒç”¨è½¨è¿¹...")
        try:
            trajectory = await self.trajectory_generator.generate_trajectory(question)
            print(f"  âœ“ è½¨è¿¹ç”Ÿæˆå®Œæˆ: {trajectory.total_tool_calls} æ¬¡å·¥å…·è°ƒç”¨")
        except Exception as e:
            print(f"  âœ— è½¨è¿¹ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        return TrajectoryDataSample(
            sample_id=f"qwen3_traj_{sample_index:05d}",
            question=question,
            topic=question_data.get("topic", ""),
            question_type=question_data.get("question_type", ""),
            trajectory=trajectory.to_dict(),
            metadata={
                "expected_search_terms": question_data.get("expected_search_terms", []),
                "tools_used": trajectory.tools_used,
                "total_tool_calls": trajectory.total_tool_calls,
                "generation_time": datetime.now().isoformat(),
                "model": self.trajectory_generator.model_name,
                "source_file": self.questions_file
            }
        )
    
    async def generate_trajectory_with_retry(
        self,
        q_data: Dict,
        sample_index: int,
        semaphore: asyncio.Semaphore,
        max_retries: int = 3
    ) -> Optional[TrajectoryDataSample]:
        """å¸¦é‡è¯•æœºåˆ¶çš„è½¨è¿¹ç”Ÿæˆ"""
        async with semaphore:
            for attempt in range(max_retries):
                try:
                    sample = await self.generate_trajectory_for_question(q_data, sample_index)
                    if sample:
                        return sample
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 2
                        print(f"  âš ï¸ [{sample_index}] å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        print(f"  â³ ç­‰å¾… {wait_time}s åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"  âœ— [{sample_index}] æ‰€æœ‰é‡è¯•å¤±è´¥: {e}")
            return None
    
    async def generate_dataset(self, concurrency: int = 5, limit: int = None) -> List[TrajectoryDataSample]:
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†
        
        Args:
            concurrency: å¹¶å‘æ•°
            limit: é™åˆ¶ç”Ÿæˆçš„é—®é¢˜æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰ï¼ŒNoneè¡¨ç¤ºç”Ÿæˆå…¨éƒ¨
        """
        print("\n" + "=" * 60)
        print("ä»å·²æœ‰é—®é¢˜ç”Ÿæˆè½¨è¿¹")
        print("=" * 60)
        print(f"æœ¬åœ°æ¨¡å‹: {self.trajectory_generator.model_name}")
        print(f"APIåœ°å€: {self.trajectory_generator.local_model_url}")
        print(f"å¹¶å‘æ•°: {concurrency}")
        
        # åŠ è½½é—®é¢˜
        questions = self.load_questions()
        
        if not questions:
            print("âœ— æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•é—®é¢˜")
            return []
        
        # å¦‚æœè®¾ç½®äº†limitï¼Œåªå¤„ç†å‰Nä¸ªé—®é¢˜
        if limit is not None:
            questions = questions[:limit]
            print(f"âš ï¸ é™åˆ¶å¤„ç†å‰ {limit} ä¸ªé—®é¢˜")
        
        # å¹¶å‘ç”Ÿæˆè½¨è¿¹
        print("\n" + "=" * 60)
        print("ç”Ÿæˆè½¨è¿¹ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰")
        print("=" * 60)
        
        semaphore = asyncio.Semaphore(concurrency)
        
        tasks = []
        for i, q_data in enumerate(questions, 1):
            task = self.generate_trajectory_with_retry(q_data, i, semaphore)
            tasks.append(task)
        
        # æ‰§è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
        samples = []
        completed = 0
        total = len(tasks)
        
        if self.incremental_save:
            print(f"ğŸ’¾ å¢é‡ä¿å­˜å·²å¯ç”¨: {self.incremental_file}")
        
        for coro in asyncio.as_completed(tasks):
            sample = await coro
            completed += 1
            if sample:
                samples.append(sample)
                # å¢é‡ä¿å­˜
                self.append_sample_to_file(sample)
            
            # æ˜¾ç¤ºè¿›åº¦
            success_rate = (len(samples) / completed * 100) if completed > 0 else 0
            print(f"\nğŸ“Š è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                  f"æˆåŠŸ: {len(samples)} | å¤±è´¥: {completed - len(samples)} | "
                  f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        self.samples = samples
        print(f"\nâœ“ å®Œæˆï¼å…±ç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬")
        if self.incremental_save:
            print(f"ğŸ’¾ æ‰€æœ‰æ ·æœ¬å·²å¢é‡ä¿å­˜åˆ°: {self.incremental_file}")
        return samples
    
    def append_sample_to_file(self, sample: TrajectoryDataSample):
        """å¢é‡ä¿å­˜ï¼šè¿½åŠ å•æ¡æ ·æœ¬åˆ°æ–‡ä»¶"""
        if not self.incremental_save:
            return
        
        try:
            with open(self.incremental_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(sample.to_dict(), ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"  âš ï¸ å¢é‡ä¿å­˜å¤±è´¥: {e}")
    
    def save_dataset(self):
        """ä¿å­˜æ•°æ®é›†ç»Ÿè®¡"""
        if not self.samples:
            print("âš ï¸ æ²¡æœ‰æ ·æœ¬å¯ä¿å­˜")
            return
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜ç»Ÿè®¡
        stats = self._generate_stats()
        stats_path = os.path.join(
            self.output_dir,
            f"trajectory_stats_{self.timestamp}_{self.model_suffix}.json"
        )
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ä¿å­˜ç»Ÿè®¡: {stats_path}")
        
        return self.incremental_file
    
    def _generate_stats(self) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.samples:
            return {}
        
        topics = {}
        question_types = {}
        tool_calls_counts = []
        
        for sample in self.samples:
            topics[sample.topic] = topics.get(sample.topic, 0) + 1
            question_types[sample.question_type] = question_types.get(sample.question_type, 0) + 1
            tool_calls_counts.append(sample.metadata.get("total_tool_calls", 0))
        
        return {
            "total_samples": len(self.samples),
            "topics": topics,
            "question_types": question_types,
            "avg_tool_calls": sum(tool_calls_counts) / len(tool_calls_counts) if tool_calls_counts else 0,
            "generation_time": datetime.now().isoformat(),
            "model": self.trajectory_generator.model_name,
            "source_file": self.questions_file
        }


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä»å·²æœ‰é—®é¢˜ç”Ÿæˆè½¨è¿¹ï¼ˆä½¿ç”¨æœ¬åœ°Qwen3-8Bï¼‰")
    parser.add_argument("--questions-file", type=str, required=True, help="é—®é¢˜JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--local-model-url", type=str, default="http://localhost:8000/v1", 
                        help="æœ¬åœ°æ¨¡å‹APIåœ°å€ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰")
    parser.add_argument("--model-name", type=str, default="Qwen3-8B", help="æ¨¡å‹åç§°")
    parser.add_argument("--output", type=str, default=OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--concurrency", type=int, default=5, help="å¹¶å‘æ•°")
    parser.add_argument("--limit", type=int, default=None, help="é™åˆ¶ç”Ÿæˆçš„é—®é¢˜æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--no-incremental", action="store_true", help="ç¦ç”¨å¢é‡ä¿å­˜")
    
    args = parser.parse_args()
    
    print(f"æœ¬åœ°æ¨¡å‹: {args.model_name}")
    print(f"APIåœ°å€: {args.local_model_url}")
    print(f"é—®é¢˜æ–‡ä»¶: {args.questions_file}")
    print(f"å¹¶å‘æ•°: {args.concurrency}")
    print(f"å¢é‡ä¿å­˜: {'ç¦ç”¨' if args.no_incremental else 'å¯ç”¨'}")
    if args.limit:
        print(f"é™åˆ¶: å‰ {args.limit} ä¸ªé—®é¢˜")
    
    generator = QuestionBasedTrajectoryGenerator(
        questions_file=args.questions_file,
        local_model_url=args.local_model_url,
        model_name=args.model_name,
        output_dir=args.output,
        incremental_save=not args.no_incremental
    )
    
    try:
        await generator.generate_dataset(concurrency=args.concurrency, limit=args.limit)
        generator.save_dataset()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²å®Œæˆçš„æ ·æœ¬...")
        if generator.samples:
            generator.save_dataset()
        print("âœ“ å·²ä¿å­˜éƒ¨åˆ†ç»“æœ")
    except Exception as e:
        print(f"\n\nâœ— ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        if generator.samples:
            print("\nä¿å­˜å·²å®Œæˆçš„æ ·æœ¬...")
            generator.save_dataset()
    
    print("\n" + "=" * 60)
    print("æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

