#!/usr/bin/env python3
"""
ä¸“é—¨æµ‹è¯•Crawl4AIBrowseToolæ˜¯å¦å¯ç”¨
åªæµ‹è¯•browse_webpageå·¥å…·ï¼Œä¸æµ‹è¯•å…¶ä»–å·¥å…·
"""

import asyncio
import os
import sys
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["MCP_TRANSPORT_PORT"] = "8003"
os.environ["MCP_TRANSPORT_HOST"] = "localhost"
os.environ["MCP_MAX_CONCURRENT_CALLS"] = "512"


def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)


def check_crawl4ai_config():
    """æ£€æŸ¥Crawl4AIç›¸å…³é…ç½®"""
    print_section("ğŸ” æ£€æŸ¥Crawl4AIé…ç½®")
    
    issues = []
    
    # æ£€æŸ¥Crawl4AIé…ç½®
    crawl4ai_vars = {
        "CRAWL4AI_API_URL": "Crawl4AI Docker API URL",
        "CRAWL4AI_API_KEY": "Crawl4AI API Key",
        "CRAWL4AI_BLOCKLIST_PATH": "Crawl4AI Blocklist Path",
    }
    
    print("ğŸ“¦ Crawl4AI Dockeré…ç½®ï¼ˆuse_ai2_config=Trueæ—¶éœ€è¦ï¼‰:")
    for var, description in crawl4ai_vars.items():
        value = os.environ.get(var)
        if value:
            print(f"âœ… {description}: {value[:50]}...")
        else:
            print(f"âš ï¸  {description}: æœªè®¾ç½®")
            if var == "CRAWL4AI_BLOCKLIST_PATH":
                issues.append(f"{var} æœªè®¾ç½®ï¼ˆuse_ai2_config=Trueæ—¶å¿…éœ€ï¼‰")
    
    print("\nğŸŒ MCPæœåŠ¡å™¨é…ç½®:")
    print(f"   Host: {os.environ.get('MCP_TRANSPORT_HOST', 'localhost')}")
    print(f"   Port: {os.environ.get('MCP_TRANSPORT_PORT', '8003')}")
    
    if issues:
        print("\nâš ï¸  é…ç½®é—®é¢˜:")
        for issue in issues:
            print(f"   - {issue}")
    
    return len(issues) == 0


async def test_mcp_server():
    """æµ‹è¯•MCPæœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    print_section("ğŸ”Œ æµ‹è¯•MCPæœåŠ¡å™¨è¿æ¥")
    
    try:
        import httpx
        host = os.environ.get("MCP_TRANSPORT_HOST", "localhost")
        port = os.environ.get("MCP_TRANSPORT_PORT", "8003")
        url = f"http://{host}:{port}/health"
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(url)
            if response.status_code == 200:
                print(f"âœ… MCPæœåŠ¡å™¨è¿è¡Œæ­£å¸¸: {url}")
                return True
            else:
                print(f"âŒ MCPæœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
                return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°MCPæœåŠ¡å™¨: {e}")
        print("\nğŸ’¡ è¯·å…ˆå¯åŠ¨MCPæœåŠ¡å™¨:")
        print(f"   cd {Path(__file__).parent / 'agent'}")
        print("   uv run python -m dr_agent.mcp_backend.main --transport http --port 8003 --host 0.0.0.0 --path /mcp")
        return False


async def test_crawl4ai_direct_url():
    """æµ‹è¯•1ï¼šç›´æ¥URLè®¿é—®"""
    print_section("ğŸŒ æµ‹è¯•1: Crawl4AI - ç›´æ¥URLè®¿é—®")
    
    try:
        from dr_agent.tool_interface.mcp_tools import Crawl4AIBrowseTool
        
        # ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„é…ç½®
        tool = Crawl4AIBrowseTool(
            tool_parser="v20250824",
            max_pages_to_fetch=1,
            timeout=180,
            name="browse_webpage",
            transport_type="StreamableHttpTransport",
            use_docker_version=True,
            use_ai2_config=True,
        )
        
        test_url = "https://en.wikipedia.org/wiki/Python_(programming_language)"
        print(f"è®¿é—®URL: {test_url}")
        print("(è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...)")
        
        result = await tool({"url": test_url})
        
        if result.error:
            print(f"âŒ é”™è¯¯: {result.error}")
            return False
        else:
            print(f"âœ… æˆåŠŸè·å–ç½‘é¡µå†…å®¹")
            for i, doc in enumerate(result.documents, 1):
                print(f"\n   é¡µé¢ {i}:")
                print(f"   URL: {doc.url}")
                if doc.text:
                    print(f"   å†…å®¹é•¿åº¦: {len(doc.text)} å­—ç¬¦")
                    print(f"   å†…å®¹é¢„è§ˆ: {doc.text[:200]}...")
                    print(f"   å†…å®¹ç»“å°¾: ...{doc.text[-100:]}")
                if doc.error:
                    print(f"   é”™è¯¯: {doc.error}")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_crawl4ai_multiple_urls():
    """æµ‹è¯•2ï¼šå¤šä¸ªURLè®¿é—®"""
    print_section("ğŸŒ æµ‹è¯•2: Crawl4AI - å¤šä¸ªURLè®¿é—®")
    
    try:
        from dr_agent.tool_interface.mcp_tools import Crawl4AIBrowseTool
        from dr_agent.tool_interface.data_types import Document, DocumentToolOutput
        
        # ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„é…ç½®
        tool = Crawl4AIBrowseTool(
            tool_parser="v20250824",
            max_pages_to_fetch=2,
            timeout=180,
            name="browse_webpage",
            transport_type="StreamableHttpTransport",
            use_docker_version=True,
            use_ai2_config=True,
        )
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆå¤šä¸ªURLï¼‰
        test_urls = [
            "https://docs.python.org/3/",
            "https://www.python.org/about/",
        ]
        
        print(f"è®¿é—® {len(test_urls)} ä¸ªURL:")
        for url in test_urls:
            print(f"  - {url}")
        print("(è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...)")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æœç´¢ç»“æœ
        documents = [
            Document(
                title=f"Test Page {i+1}",
                snippet="Test snippet",
                url=url,
                text=None,
                score=None,
            )
            for i, url in enumerate(test_urls)
        ]
        
        mock_search_result = DocumentToolOutput(
            tool_name="google_search",
            output="",
            called=True,
            error="",
            timeout=False,
            runtime=1.0,
            call_id="test-123",
            raw_output={},
            documents=documents,
            query="test query",
        )
        
        result = await tool(mock_search_result)
        
        if result.error:
            print(f"âŒ é”™è¯¯: {result.error}")
            return False
        else:
            print(f"âœ… æˆåŠŸè·å– {len(result.documents)} ä¸ªç½‘é¡µå†…å®¹")
            for i, doc in enumerate(result.documents, 1):
                print(f"\n   é¡µé¢ {i}:")
                print(f"   URL: {doc.url}")
                if doc.text:
                    print(f"   å†…å®¹é•¿åº¦: {len(doc.text)} å­—ç¬¦")
                    print(f"   å†…å®¹é¢„è§ˆ: {doc.text[:150]}...")
                if doc.error:
                    print(f"   é”™è¯¯: {doc.error}")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_crawl4ai_with_local_config():
    """æµ‹è¯•3ï¼šä½¿ç”¨æœ¬åœ°é…ç½®ï¼ˆä¸éœ€è¦Dockerï¼‰"""
    print_section("ğŸŒ æµ‹è¯•3: Crawl4AI - æœ¬åœ°é…ç½®ï¼ˆæ— Dockerï¼‰")
    
    try:
        from dr_agent.tool_interface.mcp_tools import Crawl4AIBrowseTool
        
        # ä½¿ç”¨æœ¬åœ°é…ç½®ï¼ˆä¸éœ€è¦DockeræœåŠ¡ï¼‰
        tool = Crawl4AIBrowseTool(
            tool_parser="v20250824",
            max_pages_to_fetch=1,
            timeout=180,
            name="browse_webpage",
            transport_type="StreamableHttpTransport",
            use_docker_version=False,  # ä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬
            use_ai2_config=False,      # ä¸ä½¿ç”¨AI2é…ç½®
        )
        
        test_url = "https://www.example.com/"
        print(f"è®¿é—®URL: {test_url}")
        print("(ä½¿ç”¨æœ¬åœ°Crawl4AIï¼Œä¸éœ€è¦DockeræœåŠ¡)")
        
        result = await tool({"url": test_url})
        
        if result.error:
            print(f"âŒ é”™è¯¯: {result.error}")
            return False
        else:
            print(f"âœ… æˆåŠŸè·å–ç½‘é¡µå†…å®¹")
            for i, doc in enumerate(result.documents, 1):
                print(f"\n   é¡µé¢ {i}:")
                print(f"   URL: {doc.url}")
                if doc.text:
                    print(f"   å†…å®¹é•¿åº¦: {len(doc.text)} å­—ç¬¦")
                    print(f"   å†…å®¹é¢„è§ˆ: {doc.text[:200]}...")
                if doc.error:
                    print(f"   é”™è¯¯: {doc.error}")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print_section("ğŸš€ Crawl4AIBrowseTool ä¸“é¡¹æµ‹è¯•")
    
    # 1. æ£€æŸ¥é…ç½®
    check_crawl4ai_config()
    
    # 2. æ£€æŸ¥MCPæœåŠ¡å™¨
    server_ok = await test_mcp_server()
    
    if not server_ok:
        print_section("âŒ æµ‹è¯•ç»ˆæ­¢ï¼šMCPæœåŠ¡å™¨æœªè¿è¡Œ")
        return
    
    # 3. è¿è¡Œæµ‹è¯•
    results = {}
    
    print("\n" + "ğŸ’¡" * 30)
    print("å¼€å§‹æµ‹è¯•Crawl4AIBrowseTool...")
    print("å¦‚æœé…ç½®äº†DockeræœåŠ¡ï¼Œå°†æµ‹è¯•Dockerç‰ˆæœ¬")
    print("å¦‚æœæ²¡æœ‰ï¼Œå°†æµ‹è¯•æœ¬åœ°ç‰ˆæœ¬")
    print("ğŸ’¡" * 30)
    
    # å°è¯•Dockerç‰ˆæœ¬
    has_docker = os.environ.get("CRAWL4AI_API_URL") is not None
    
    if has_docker:
        print("\næ£€æµ‹åˆ°CRAWL4AI_API_URLï¼Œå°†æµ‹è¯•Dockerç‰ˆæœ¬")
        results['test1_direct'] = await test_crawl4ai_direct_url()
        results['test2_multiple'] = await test_crawl4ai_multiple_urls()
    else:
        print("\næœªæ£€æµ‹åˆ°CRAWL4AI_API_URLï¼Œå°†æµ‹è¯•æœ¬åœ°ç‰ˆæœ¬")
        results['test3_local'] = await test_crawl4ai_with_local_config()
    
    # 4. æ€»ç»“
    print_section("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    
    for test_name, success in results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:20s}: {status}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\nğŸ‰ Crawl4AIBrowseTool æµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ¨ è®­ç»ƒæ—¶browse_webpageå·¥å…·åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        print("\nğŸ’¡ å¸¸è§é—®é¢˜:")
        print("   1. å¦‚æœä½¿ç”¨Dockerç‰ˆæœ¬ï¼š")
        print("      - ç¡®ä¿CRAWL4AI_API_URLæ­£ç¡®")
        print("      - ç¡®ä¿CRAWL4AI_BLOCKLIST_PATHæŒ‡å‘æœ‰æ•ˆæ–‡ä»¶")
        print("   2. å¦‚æœä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬ï¼š")
        print("      - ç¡®ä¿crawl4aiåŒ…å·²å®‰è£…: pip install crawl4ai")
        print("   3. ç¡®ä¿MCPæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
    
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

