#!/usr/bin/env python3
"""
æµ‹è¯•Crawl4AIBrowseToolæ˜¯å¦å¯ç”¨
æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„å·¥å…·è°ƒç”¨æ–¹å¼
"""

import asyncio
import os
import sys
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
os.environ["MCP_TRANSPORT_PORT"] = "8003"
os.environ["MCP_TRANSPORT_HOST"] = "localhost"
os.environ["MCP_MAX_CONCURRENT_CALLS"] = "512"

# API Keys
os.environ["S2_API_KEY"] = "sk-user-F788DB8EABBDAD1858E82734A4E0C1BA"
os.environ["SERPER_API_KEY"] = "56e20b0fb1dc8a9d19fb80be90fb346e63294148"


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print("=" * 60)
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    print("=" * 60)
    
    issues = []
    
    # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    required_vars = {
        "S2_API_KEY": "Semantic Scholar API Key",
        "SERPER_API_KEY": "Serper API Key (ç”¨äºGoogleæœç´¢)",
    }
    
    for var, description in required_vars.items():
        value = os.environ.get(var)
        if value:
            print(f"âœ… {description}: {value[:10]}...")
        else:
            print(f"âŒ {description}: æœªè®¾ç½®")
            issues.append(f"ç¯å¢ƒå˜é‡ {var} æœªè®¾ç½®")
    
    # æ£€æŸ¥Crawl4AIé…ç½®ï¼ˆuse_ai2_config=Trueæ—¶éœ€è¦ï¼‰
    crawl4ai_vars = {
        "CRAWL4AI_API_URL": "Crawl4AI Docker API URL",
        "CRAWL4AI_API_KEY": "Crawl4AI API Key",
        "CRAWL4AI_BLOCKLIST_PATH": "Crawl4AI Blocklist Path",
    }
    
    print("\nğŸ“¦ Crawl4AIé…ç½®ï¼ˆuse_ai2_config=Trueæ—¶éœ€è¦ï¼‰:")
    for var, description in crawl4ai_vars.items():
        value = os.environ.get(var)
        if value:
            print(f"âœ… {description}: {value[:50]}...")
        else:
            print(f"âš ï¸  {description}: æœªè®¾ç½®")
            issues.append(f"Crawl4AIé…ç½® {var} æœªè®¾ç½®ï¼ˆå¦‚æœä½¿ç”¨use_ai2_config=Trueåˆ™å¿…éœ€ï¼‰")
    
    print("\nğŸŒ MCPæœåŠ¡å™¨é…ç½®:")
    print(f"   Host: {os.environ.get('MCP_TRANSPORT_HOST', 'localhost')}")
    print(f"   Port: {os.environ.get('MCP_TRANSPORT_PORT', '8003')}")
    
    if issues:
        print("\nâš ï¸  å‘ç°é…ç½®é—®é¢˜:")
        for issue in issues:
            print(f"   - {issue}")
        print("\nğŸ’¡ è§£å†³æ–¹æ³•è§ä¸‹æ–¹çš„'é…ç½®è¯´æ˜'éƒ¨åˆ†")
    else:
        print("\nâœ… ç¯å¢ƒé…ç½®æ£€æŸ¥é€šè¿‡ï¼")
    
    return len(issues) == 0


async def test_mcp_server():
    """æµ‹è¯•MCPæœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    print("\n" + "=" * 60)
    print("ğŸ”Œ æµ‹è¯•MCPæœåŠ¡å™¨è¿æ¥...")
    print("=" * 60)
    
    try:
        import httpx
        host = os.environ.get("MCP_TRANSPORT_HOST", "localhost")
        port = os.environ.get("MCP_TRANSPORT_PORT", "8003")
        url = f"http://{host}:{port}/health"
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(url)
            if response.status_code == 200:
                print(f"âœ… MCPæœåŠ¡å™¨è¿è¡Œæ­£å¸¸: {url}")
                return True
            else:
                print(f"âŒ MCPæœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
                return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°MCPæœåŠ¡å™¨: {e}")
        print("\nğŸ’¡ è¯·å…ˆå¯åŠ¨MCPæœåŠ¡å™¨:")
        print(f"   cd {Path(__file__).parent / 'agent'}")
        print("   uv run python -m dr_agent.mcp_backend.main --transport http --port 8003 --host 0.0.0.0 --path /mcp")
        return False


async def test_snippet_search():
    """æµ‹è¯•snippet_searchå·¥å…·ï¼ˆSemantic Scholarï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ“š æµ‹è¯• snippet_search (Semantic Scholar)")
    print("=" * 60)
    
    try:
        from dr_agent.tool_interface.mcp_tools import SemanticScholarSnippetSearchTool
        
        tool = SemanticScholarSnippetSearchTool(
            tool_parser="v20250824",
            number_documents_to_search=3,
            timeout=60,
            name="snippet_search",
            transport_type="StreamableHttpTransport",
        )
        
        query = "large language model"
        print(f"æŸ¥è¯¢: {query}")
        
        result = await tool({"query": query, "limit": 3})
        
        if result.error:
            print(f"âŒ é”™è¯¯: {result.error}")
            return False
        else:
            print(f"âœ… æˆåŠŸè·å– {len(result.documents)} ä¸ªç»“æœ")
            for i, doc in enumerate(result.documents[:2], 1):
                print(f"\n   ç»“æœ {i}:")
                print(f"   æ ‡é¢˜: {doc.title[:80]}...")
                print(f"   ç‰‡æ®µ: {doc.snippet[:100]}...")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_google_search():
    """æµ‹è¯•google_searchå·¥å…·ï¼ˆSerperï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯• google_search (Serper)")
    print("=" * 60)
    
    try:
        from dr_agent.tool_interface.mcp_tools import SerperSearchTool
        
        tool = SerperSearchTool(
            tool_parser="v20250824",
            number_documents_to_search=5,
            timeout=60,
            name="google_search",
            transport_type="StreamableHttpTransport",
        )
        
        query = "python programming tutorial"
        print(f"æŸ¥è¯¢: {query}")
        
        result = await tool({"query": query})
        
        if result.error:
            print(f"âŒ é”™è¯¯: {result.error}")
            return False
        else:
            print(f"âœ… æˆåŠŸè·å– {len(result.documents)} ä¸ªç»“æœ")
            for i, doc in enumerate(result.documents[:3], 1):
                print(f"\n   ç»“æœ {i}:")
                print(f"   æ ‡é¢˜: {doc.title[:80]}")
                print(f"   URL: {doc.url}")
                print(f"   æ‘˜è¦: {doc.snippet[:100]}...")
            
            # è¿”å›ç»“æœä¾›browse_webpageæµ‹è¯•ä½¿ç”¨
            return result
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_browse_webpage(search_result=None):
    """æµ‹è¯•browse_webpageå·¥å…·ï¼ˆCrawl4AIï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸŒ æµ‹è¯• browse_webpage (Crawl4AI)")
    print("=" * 60)
    
    try:
        from dr_agent.tool_interface.mcp_tools import Crawl4AIBrowseTool
        
        # ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„é…ç½®
        tool = Crawl4AIBrowseTool(
            tool_parser="v20250824",
            max_pages_to_fetch=2,
            timeout=180,
            name="browse_webpage",
            transport_type="StreamableHttpTransport",
            use_docker_version=True,  # è®­ç»ƒè„šæœ¬ä½¿ç”¨Dockerç‰ˆæœ¬
            use_ai2_config=True,      # è®­ç»ƒè„šæœ¬ä½¿ç”¨AI2é…ç½®
        )
        
        # æµ‹è¯•æ–¹å¼1ï¼šç›´æ¥URL
        test_url = "https://en.wikipedia.org/wiki/Python_(programming_language)"
        print(f"\næµ‹è¯•1 - ç›´æ¥URL: {test_url}")
        
        result = await tool({"url": test_url})
        
        if result.error:
            print(f"âŒ é”™è¯¯: {result.error}")
            return False
        else:
            print(f"âœ… æˆåŠŸè·å–ç½‘é¡µå†…å®¹")
            for i, doc in enumerate(result.documents, 1):
                print(f"\n   é¡µé¢ {i}:")
                print(f"   URL: {doc.url}")
                if doc.text:
                    print(f"   å†…å®¹é•¿åº¦: {len(doc.text)} å­—ç¬¦")
                    print(f"   å†…å®¹é¢„è§ˆ: {doc.text[:150]}...")
                if doc.error:
                    print(f"   é”™è¯¯: {doc.error}")
        
        # æµ‹è¯•æ–¹å¼2ï¼šä½¿ç”¨æœç´¢ç»“æœï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„é“¾å¼è°ƒç”¨ï¼‰
        if search_result and hasattr(search_result, 'documents'):
            print(f"\næµ‹è¯•2 - ä»æœç´¢ç»“æœè·å–URL:")
            result2 = await tool(search_result)
            
            if result2.error:
                print(f"âŒ é”™è¯¯: {result2.error}")
            else:
                print(f"âœ… æˆåŠŸè·å– {len(result2.documents)} ä¸ªç½‘é¡µå†…å®¹")
                for i, doc in enumerate(result2.documents[:2], 1):
                    print(f"\n   é¡µé¢ {i}:")
                    print(f"   URL: {doc.url}")
                    if doc.text:
                        print(f"   å†…å®¹é•¿åº¦: {len(doc.text)} å­—ç¬¦")
        
        return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "=" * 60)
    print("ğŸš€ Dr-Tulu å·¥å…·æµ‹è¯•è„šæœ¬ï¼ˆå®Œæ•´é›†æˆæµ‹è¯•ï¼‰")
    print("=" * 60)
    
    print("\nğŸ’¡ è¯´æ˜ï¼š")
    print("   æœ¬è„šæœ¬æµ‹è¯•è®­ç»ƒæ—¶ä½¿ç”¨çš„æ‰€æœ‰3ä¸ªå·¥å…·ï¼š")
    print("   1. snippet_search (Semantic Scholar)")
    print("   2. google_search (Serper)")
    print("   3. browse_webpage (Crawl4AI)")
    print("\n   å¦‚æœåªæƒ³æµ‹è¯•Crawl4AIï¼Œè¯·è¿è¡Œ: python test_crawl4ai_only.py")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    env_ok = check_environment()
    
    # 2. æ£€æŸ¥MCPæœåŠ¡å™¨
    server_ok = await test_mcp_server()
    
    if not server_ok:
        print("\n" + "=" * 60)
        print("âŒ æµ‹è¯•ç»ˆæ­¢ï¼šMCPæœåŠ¡å™¨æœªè¿è¡Œ")
        print("=" * 60)
        return
    
    # 3. æµ‹è¯•å·¥å…·
    results = {}
    
    # æµ‹è¯•snippet_search
    results['snippet_search'] = await test_snippet_search()
    
    # æµ‹è¯•google_search
    search_result = await test_google_search()
    results['google_search'] = search_result is not False
    
    # æµ‹è¯•browse_webpage
    results['browse_webpage'] = await test_browse_webpage(search_result)
    
    # 4. æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    for tool_name, success in results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{tool_name:20s}: {status}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰å·¥å…·æµ‹è¯•é€šè¿‡ï¼è®­ç»ƒç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†å·¥å…·æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
    
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

