"""
æµ‹è¯•é€šè¿‡ MCP è°ƒç”¨ browse_webpage (Docker ç‰ˆæœ¬)
"""
import asyncio
import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "scripts" / "pubmed_data_generator"))

from fastmcp import Client


async def test_mcp_browse_webpage():
    """æµ‹è¯• MCP æ–¹å¼è°ƒç”¨ browse_webpage (Docker ç‰ˆæœ¬)"""
    
    # è¯»å– MCP é…ç½®
    mcp_host = os.environ.get("MCP_TRANSPORT_HOST", "127.0.0.1")
    mcp_port = os.environ.get("MCP_TRANSPORT_PORT", "8003")
    mcp_url = f"http://{mcp_host}:{mcp_port}/mcp"
    
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• MCP browse_webpage (Docker ç‰ˆæœ¬)")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("ğŸ“‹ ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    print(f"  MCP_HOST: {mcp_host}")
    print(f"  MCP_PORT: {mcp_port}")
    print(f"  MCP_URL: {mcp_url}")
    
    crawl4ai_vars = {
        "CRAWL4AI_API_URL": os.environ.get("CRAWL4AI_API_URL"),
        "CRAWL4AI_API_KEY": os.environ.get("CRAWL4AI_API_KEY"),
        "CRAWL4AI_BLOCKLIST_PATH": os.environ.get("CRAWL4AI_BLOCKLIST_PATH"),
    }
    
    print(f"\n  Crawl4AI é…ç½®:")
    for key, value in crawl4ai_vars.items():
        if value:
            display_value = value[:50] + "..." if len(value) > 50 else value
            print(f"    âœ… {key}: {display_value}")
        else:
            print(f"    âš ï¸  {key}: æœªè®¾ç½®")
    print()
    
    # åˆ›å»º MCP å®¢æˆ·ç«¯
    client = Client(mcp_url, timeout=120)
    
    try:
        print("ğŸ”Œ è¿æ¥åˆ° MCP æœåŠ¡å™¨...")
        async with client:
            # æµ‹è¯•å·¥å…·è°ƒç”¨
            test_url = "https://en.wikipedia.org/wiki/Python_(programming_language)"
            
            print(f"ğŸ“„ æµ‹è¯• URL: {test_url}")
            print("â³ è°ƒç”¨å·¥å…·: crawl4ai_docker_fetch_webpage_content")
            print("   (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...)")
            print()
            
            # è°ƒç”¨å·¥å…·
            result = await client.call_tool(
                "crawl4ai_docker_fetch_webpage_content",
                {
                    "url": test_url,
                    "use_ai2_config": True,  # ä½¿ç”¨ AI2 é…ç½®
                    "ignore_links": True,
                    "bypass_cache": True,
                }
            )
            
            # è§£æç»“æœ
            if hasattr(result, "content") and result.content:
                if hasattr(result.content[0], "text"):
                    raw_result = json.loads(result.content[0].text)
                else:
                    raw_result = {"data": str(result.content[0])}
            else:
                raw_result = {"error": "No content in response"}
            
            print("=" * 60)
            print("ğŸ“Š ç»“æœåˆ†æ")
            print("=" * 60)
            
            if "error" in raw_result:
                print(f"âŒ è°ƒç”¨å¤±è´¥: {raw_result['error']}")
                return False
            
            # æ˜¾ç¤ºç»“æœä¿¡æ¯
            print(f"âœ… è°ƒç”¨æˆåŠŸï¼")
            print()
            
            print(f"ğŸ“Œ è¿”å›å­—æ®µ:")
            for key in raw_result.keys():
                print(f"   - {key}")
            print()
            
            # æ˜¾ç¤º URL
            if "url" in raw_result:
                print(f"ğŸ”— URL: {raw_result['url']}")
            
            # æ˜¾ç¤ºæˆåŠŸçŠ¶æ€
            if "success" in raw_result:
                status = "âœ… æˆåŠŸ" if raw_result["success"] else "âŒ å¤±è´¥"
                print(f"ğŸ“Š çŠ¶æ€: {status}")
            
            # æ˜¾ç¤ºå†…å®¹é•¿åº¦
            if "markdown" in raw_result:
                markdown_len = len(raw_result["markdown"])
                print(f"ğŸ“ Markdown é•¿åº¦: {markdown_len:,} å­—ç¬¦")
                print(f"ğŸ“„ å†…å®¹é¢„è§ˆ (å‰ 300 å­—ç¬¦):")
                print("-" * 60)
                print(raw_result["markdown"][:300])
                print("-" * 60)
                print(f"ğŸ“„ å†…å®¹ç»“å°¾ (å 200 å­—ç¬¦):")
                print("-" * 60)
                print(raw_result["markdown"][-200:])
                print("-" * 60)
            
            if "fit_markdown" in raw_result and raw_result["fit_markdown"]:
                fit_len = len(raw_result["fit_markdown"])
                print(f"âœ‚ï¸  Fit Markdown é•¿åº¦: {fit_len:,} å­—ç¬¦")
            
            # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "error" in raw_result and raw_result["error"]:
                print(f"âš ï¸  é”™è¯¯ä¿¡æ¯: {raw_result['error']}")
            
            print()
            print("=" * 60)
            print("âœ… æµ‹è¯•å®Œæˆï¼browse_webpage (Docker ç‰ˆæœ¬) å·¥ä½œæ­£å¸¸")
            print("=" * 60)
            
            return True
            
    except Exception as e:
        print("=" * 60)
        print(f"âŒ æµ‹è¯•å¤±è´¥")
        print("=" * 60)
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print()
        
        # ç»™å‡ºæ’æŸ¥å»ºè®®
        print("ğŸ’¡ æ’æŸ¥å»ºè®®:")
        print("1. ç¡®ä¿ MCP æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ:")
        print(f"   curl http://{mcp_host}:{mcp_port}/health")
        print()
        print("2. ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®:")
        print("   export CRAWL4AI_API_URL='http://localhost:11235'")
        print("   export CRAWL4AI_API_KEY='mamba-out'")
        print("   export CRAWL4AI_BLOCKLIST_PATH='/path/to/blocklist.txt'")
        print()
        print("3. ç¡®ä¿ Crawl4AI Docker å®¹å™¨æ­£åœ¨è¿è¡Œ:")
        print("   docker ps | grep crawl4ai")
        print()
        
        return False


if __name__ == "__main__":
    success = asyncio.run(test_mcp_browse_webpage())
    sys.exit(0 if success else 1)

